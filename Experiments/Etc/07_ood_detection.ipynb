{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# custom modules\n",
    "from utils import set_seed, get_clf_eval, make_submission, record_experimental_results\n",
    "import preprocessing as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\"seed\": 33, \n",
    "           \"batch_size\": 32, \"shuffle\": True,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(hparams[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "tr_data, tt_data = pp.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연속형 / 범주형 변수 이름명 분리하기\n",
    "cont_feats, cat_feats = [], []\n",
    "for col_name in tr_data.columns:\n",
    "    if tr_data[col_name].dtype == object:\n",
    "        cat_feats.append(col_name)\n",
    "    else:\n",
    "        cont_feats.append(col_name)\n",
    "\n",
    "print(cont_feats, len(cont_feats))\n",
    "print(cat_feats, len(cat_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 변수는 label encoding하기\n",
    "tr_data, tt_data = pp.label_encoding(tr_data, tt_data, features=cat_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 불필요한 feature 삭제\n",
    "target = set(tr_data.columns) - set(['customer_country', 'customer_idx', 'lead_desc_length', 'lead_owner', 'is_converted'])\n",
    "tr_data, tt_data = pp.delete_features(tr_data, tt_data, features=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치는 0으로 채우기\n",
    "tr_data = tr_data.fillna(0)\n",
    "tt_data = tt_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize하기\n",
    "def normalize(df, cont_feats):\n",
    "    for feat_name in cont_feats:\n",
    "        if df[feat_name].min() >= 0. and df[feat_name].max() <= 1.:\n",
    "            continue\n",
    "        \n",
    "        # max scaling\n",
    "        df[feat_name] = df[feat_name] / df[feat_name].max()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = normalize(tr_data, ['customer_country', 'customer_idx', 'lead_desc_length', 'lead_owner'])\n",
    "tt_data = normalize(tt_data, ['customer_country', 'customer_idx', 'lead_desc_length', 'lead_owner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train / validation data split 하기 (+ positive sample을 validation data에 추가하기)\n",
    "tr_data_neg = tr_data[tr_data['is_converted'] == False]\n",
    "tr_data_pos = tr_data[tr_data['is_converted'] == True]\n",
    "\n",
    "x_tr, y_tr, x_val, y_val = pp.split_train_and_validation(tr_data_neg, seed=hparams['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: tensor data로 바꾸기\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, x: pd.DataFrame, y: pd.Series):\n",
    "        super().__init__()\n",
    "\n",
    "        self.x = torch.tensor(x.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg = TabularDataset(x_tr, y_tr)\n",
    "train_pos = TabularDataset(tr_data_pos.drop(['is_converted'], axis=1), tr_data_pos['is_converted'])\n",
    "\n",
    "validation = TabularDataset(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tt = tt_data.drop(['is_converted', 'id'], axis=1)\n",
    "test = TabularDataset(x_tt, tt_data['is_converted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Dataloader 선언\n",
    "tr_neg_loader = DataLoader(train_neg, batch_size=hparams['batch_size'], shuffle=hparams['shuffle'],\n",
    "                       pin_memory=True, drop_last=False)\n",
    "\n",
    "tr_pos_loader = DataLoader(train_pos, batch_size=1, shuffle=False, pin_memory=True, drop_last=False)\n",
    "val_loader = DataLoader(validation, batch_size=1, shuffle=False, pin_memory=True, drop_last=False)\n",
    "tt_loader = DataLoader(test, batch_size=1, shuffle=False, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement architecture\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim, bias=True, dtype=torch.float32),\n",
    "            nn.BatchNorm1d(num_features=out_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, n_features=28, n_layers=4):\n",
    "        super().__init__()\n",
    "\n",
    "        in_dim = n_features\n",
    "        self.encoder = nn.Sequential()\n",
    "        for i in range(n_layers):\n",
    "            if i <= 1:\n",
    "                out_dim = in_dim\n",
    "            else:\n",
    "                out_dim = in_dim - 1\n",
    "\n",
    "            self.encoder.append(MLPBlock(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "\n",
    "        self.decoder = nn.Sequential()\n",
    "        for i in range(n_layers):\n",
    "            if i >= 1:\n",
    "                out_dim = n_features\n",
    "            else:\n",
    "                out_dim = in_dim + 1\n",
    "                \n",
    "            self.decoder.append(MLPBlock(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(n_features=4, n_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: criterion & optimizer\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.0009)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: training\n",
    "epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i + 1} | \", end=\"\")\n",
    "\n",
    "    # negative samples\n",
    "    # tr_losses = []\n",
    "    tr_loss_per_epoch = []\n",
    "    model.train()\n",
    "    for tr_x, _ in tr_neg_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        reconstructed_x = model(tr_x)\n",
    "\n",
    "        tr_loss = criterion(reconstructed_x, tr_x)\n",
    "        tr_loss_per_epoch.append(tr_loss.item())\n",
    "\n",
    "        tr_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_tr_mse_loss = sum(tr_loss_per_epoch) / len(tr_loss_per_epoch)\n",
    "    print(f'training loss [negative]: {avg_tr_mse_loss:2.6f} |', end=' ')\n",
    "    # tr_losses.append(avg_tr_mse_loss)\n",
    "\n",
    "    val_losses = []\n",
    "    val_loss_per_epoch = []\n",
    "    model.eval()\n",
    "    for val_x, _ in tt_loader:\n",
    "        reconstructed_x = model(val_x)\n",
    "\n",
    "        val_loss = criterion(reconstructed_x, val_x)\n",
    "        val_loss_per_epoch.append(val_loss.item())\n",
    "\n",
    "    avg_val_mse_loss = sum(val_loss_per_epoch) / len(val_loss_per_epoch)\n",
    "    print(f'validation loss: {avg_val_mse_loss:2.6f}')\n",
    "    val_losses.append(avg_val_mse_loss)\n",
    "\n",
    "    # if avg_val_mse_loss < best_val_loss:\n",
    "    #     best_val_loss = avg_val_mse_loss\n",
    "    # else:\n",
    "    #     print(f'Early Stopping')\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check mse of positive training samples\n",
    "mse_of_pos_sample = []\n",
    "model.eval()\n",
    "for x_pos, y in tr_pos_loader:\n",
    "    reconstructed_x = model(x_pos)\n",
    "    mse_loss = criterion(reconstructed_x, x_pos)\n",
    "    mse_of_pos_sample.append(mse_loss)\n",
    "\n",
    "print(max(mse_of_pos_sample))\n",
    "print(min(mse_of_pos_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mse_of_pos_sample[len(mse_of_pos_sample) // 2 + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test inference\n",
    "threshold = 0.002\n",
    "\n",
    "tt_loader = DataLoader(test, batch_size=1, shuffle=False, pin_memory=True, drop_last=False)\n",
    "model.eval()\n",
    "is_converted = []\n",
    "for x_tt, _ in tt_loader:\n",
    "    reconstructed_x = model(x_tt)\n",
    "    mse_loss = criterion(reconstructed_x, x_tt)\n",
    "\n",
    "    # negative sample만을 학습한 모델에 데이터를 입력했을 때, 설정한 threshold 값보다 높다면 -> positive sample\n",
    "    if mse_loss.item() >= threshold:\n",
    "        is_converted.append(True)\n",
    "\n",
    "    # threshold보다 낮다면 -> negative sample\n",
    "    else:\n",
    "        is_converted.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = np.array(is_converted)\n",
    "sum(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ood_detection_prototype_tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(dir_name='05_ood_detection',\n",
    "                y_pred=y_test_pred,\n",
    "                model_name=model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lgaimers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
