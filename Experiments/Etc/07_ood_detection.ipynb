{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# custom modules\n",
    "from utils import set_seed, get_clf_eval, make_submission, record_experimental_results\n",
    "import preprocessing as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\"seed\": 33, \n",
    "           \"batch_size\": 32, \"shuffle\": True,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(hparams[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "tr_data, tt_data = pp.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bant_submit', 'com_reg_ver_win_rate', 'customer_idx', 'historical_existing_cnt', 'id_strategic_ver', 'it_strategic_ver', 'idit_strategic_ver', 'lead_desc_length', 'ver_cus', 'ver_pro', 'ver_win_rate_x', 'ver_win_ratio_per_bu', 'lead_owner', 'is_converted'] 14\n",
      "['customer_country', 'business_unit', 'customer_type', 'enterprise', 'customer_job', 'inquiry_type', 'product_category', 'product_subcategory', 'product_modelname', 'customer_country.1', 'customer_position', 'response_corporate', 'expected_timeline', 'business_area', 'business_subarea'] 15\n"
     ]
    }
   ],
   "source": [
    "# 연속형 / 범주형 변수 이름명 분리하기\n",
    "cont_feats, cat_feats = [], []\n",
    "for col_name in tr_data.columns:\n",
    "    if tr_data[col_name].dtype == object:\n",
    "        cat_feats.append(col_name)\n",
    "    else:\n",
    "        cont_feats.append(col_name)\n",
    "\n",
    "print(cont_feats, len(cont_feats))\n",
    "print(cat_feats, len(cat_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 변수는 label encoding하기\n",
    "tr_data, tt_data = pp.label_encoding(tr_data, tt_data, features=cat_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 불필요한 feature 삭제\n",
    "target = set(tr_data.columns) - set(['customer_country', 'customer_idx', 'lead_desc_length', 'lead_owner', 'is_converted'])\n",
    "tr_data, tt_data = pp.delete_features(tr_data, tt_data, features=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치는 0으로 채우기\n",
    "tr_data = tr_data.fillna(0)\n",
    "tt_data = tt_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize하기\n",
    "def normalize(df, cont_feats):\n",
    "    for feat_name in cont_feats:\n",
    "        if df[feat_name].min() >= 0. and df[feat_name].max() <= 1.:\n",
    "            continue\n",
    "        \n",
    "        # max scaling\n",
    "        df[feat_name] = df[feat_name] / df[feat_name].max()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = normalize(tr_data, ['customer_country', 'customer_idx', 'lead_desc_length', 'lead_owner'])\n",
    "tt_data = normalize(tt_data, ['customer_country', 'customer_idx', 'lead_desc_length', 'lead_owner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train / validation data split 하기 (+ positive sample을 validation data에 추가하기)\n",
    "tr_data_neg = tr_data[tr_data['is_converted'] == False]\n",
    "tr_data_pos = tr_data[tr_data['is_converted'] == True]\n",
    "\n",
    "x_tr, y_tr, x_val, y_val = pp.split_train_and_validation(tr_data_neg, seed=hparams['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: tensor data로 바꾸기\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, x: pd.DataFrame, y: pd.Series):\n",
    "        super().__init__()\n",
    "\n",
    "        self.x = torch.tensor(x.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg = TabularDataset(x_tr, y_tr)\n",
    "train_pos = TabularDataset(tr_data_pos.drop(['is_converted'], axis=1), tr_data_pos['is_converted'])\n",
    "\n",
    "validation = TabularDataset(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tt = tt_data.drop(['is_converted', 'id'], axis=1)\n",
    "test = TabularDataset(x_tt, tt_data['is_converted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Dataloader 선언\n",
    "tr_neg_loader = DataLoader(train_neg, batch_size=hparams['batch_size'], shuffle=hparams['shuffle'],\n",
    "                       pin_memory=True, drop_last=False)\n",
    "\n",
    "tr_pos_loader = DataLoader(train_pos, batch_size=1, shuffle=False, pin_memory=True, drop_last=False)\n",
    "val_loader = DataLoader(validation, batch_size=1, shuffle=False, pin_memory=True, drop_last=False)\n",
    "tt_loader = DataLoader(test, batch_size=1, shuffle=False, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement architecture\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim, bias=True, dtype=torch.float32),\n",
    "            nn.BatchNorm1d(num_features=out_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, n_features=28, n_layers=4):\n",
    "        super().__init__()\n",
    "\n",
    "        in_dim = n_features\n",
    "        self.encoder = nn.Sequential()\n",
    "        for i in range(n_layers):\n",
    "            if i <= 1:\n",
    "                out_dim = in_dim\n",
    "            else:\n",
    "                out_dim = in_dim - 1\n",
    "\n",
    "            self.encoder.append(MLPBlock(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "\n",
    "        self.decoder = nn.Sequential()\n",
    "        for i in range(n_layers):\n",
    "            if i >= 1:\n",
    "                out_dim = n_features\n",
    "            else:\n",
    "                out_dim = in_dim + 1\n",
    "                \n",
    "            self.decoder.append(MLPBlock(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(n_features=4, n_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: criterion & optimizer\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.0009)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | training loss [negative]: 0.087161 | validation loss: 0.042552\n",
      "Epoch 2 | training loss [negative]: 0.036621 | validation loss: 0.034649\n",
      "Epoch 3 | training loss [negative]: 0.029973 | validation loss: 0.033193\n",
      "Epoch 4 | training loss [negative]: 0.027086 | validation loss: 0.028184\n",
      "Epoch 5 | training loss [negative]: 0.022066 | validation loss: 0.024160\n",
      "Epoch 6 | training loss [negative]: 0.019081 | validation loss: 0.021184\n",
      "Epoch 7 | training loss [negative]: 0.017443 | validation loss: 0.019321\n",
      "Epoch 8 | training loss [negative]: 0.016446 | validation loss: 0.020355\n",
      "Epoch 9 | training loss [negative]: 0.015934 | validation loss: 0.017424\n",
      "Epoch 10 | training loss [negative]: 0.015732 | validation loss: 0.018463\n",
      "Epoch 11 | training loss [negative]: 0.015746 | validation loss: 0.016326\n",
      "Epoch 12 | training loss [negative]: 0.015428 | validation loss: 0.018487\n",
      "Epoch 13 | training loss [negative]: 0.015405 | validation loss: 0.017814\n",
      "Epoch 14 | training loss [negative]: 0.015416 | validation loss: 0.017600\n",
      "Epoch 15 | training loss [negative]: 0.015297 | validation loss: 0.016719\n",
      "Epoch 16 | training loss [negative]: 0.015396 | validation loss: 0.016319\n",
      "Epoch 17 | training loss [negative]: 0.015333 | validation loss: 0.017214\n",
      "Epoch 18 | training loss [negative]: 0.015227 | validation loss: 0.017623\n",
      "Epoch 19 | training loss [negative]: 0.015101 | validation loss: 0.019575\n",
      "Epoch 20 | training loss [negative]: 0.015029 | validation loss: 0.016279\n",
      "Epoch 21 | training loss [negative]: 0.013870 | validation loss: 0.013772\n",
      "Epoch 22 | training loss [negative]: 0.013343 | validation loss: 0.013613\n",
      "Epoch 23 | training loss [negative]: 0.013103 | validation loss: 0.016822\n",
      "Epoch 24 | training loss [negative]: 0.012398 | validation loss: 0.011864\n",
      "Epoch 25 | training loss [negative]: 0.011333 | validation loss: 0.010534\n",
      "Epoch 26 | training loss [negative]: 0.010708 | validation loss: 0.009760\n",
      "Epoch 27 | training loss [negative]: 0.010338 | validation loss: 0.009736\n",
      "Epoch 28 | training loss [negative]: 0.009646 | validation loss: 0.010516\n",
      "Epoch 29 | training loss [negative]: 0.009354 | validation loss: 0.011586\n",
      "Epoch 30 | training loss [negative]: 0.009242 | validation loss: 0.010008\n",
      "Epoch 31 | training loss [negative]: 0.008993 | validation loss: 0.010333\n",
      "Epoch 32 | training loss [negative]: 0.008996 | validation loss: 0.011935\n",
      "Epoch 33 | training loss [negative]: 0.008859 | validation loss: 0.010787\n",
      "Epoch 34 | training loss [negative]: 0.008845 | validation loss: 0.013111\n",
      "Epoch 35 | training loss [negative]: 0.008783 | validation loss: 0.009562\n",
      "Epoch 36 | training loss [negative]: 0.008834 | validation loss: 0.010548\n",
      "Epoch 37 | training loss [negative]: 0.008945 | validation loss: 0.011833\n",
      "Epoch 38 | training loss [negative]: 0.008857 | validation loss: 0.010589\n",
      "Epoch 39 | training loss [negative]: 0.008925 | validation loss: 0.013645\n",
      "Epoch 40 | training loss [negative]: 0.008786 | validation loss: 0.009741\n",
      "Epoch 41 | training loss [negative]: 0.008657 | validation loss: 0.010475\n",
      "Epoch 42 | training loss [negative]: 0.008658 | validation loss: 0.013703\n",
      "Epoch 43 | training loss [negative]: 0.008698 | validation loss: 0.010298\n",
      "Epoch 44 | training loss [negative]: 0.008762 | validation loss: 0.012104\n",
      "Epoch 45 | training loss [negative]: 0.008660 | validation loss: 0.010384\n",
      "Epoch 46 | training loss [negative]: 0.008682 | validation loss: 0.012586\n",
      "Epoch 47 | training loss [negative]: 0.008562 | validation loss: 0.010731\n",
      "Epoch 48 | training loss [negative]: 0.008607 | validation loss: 0.010171\n",
      "Epoch 49 | training loss [negative]: 0.008684 | validation loss: 0.011238\n",
      "Epoch 50 | training loss [negative]: 0.008657 | validation loss: 0.011780\n",
      "Epoch 51 | training loss [negative]: 0.008574 | validation loss: 0.010299\n",
      "Epoch 52 | training loss [negative]: 0.008590 | validation loss: 0.009684\n",
      "Epoch 53 | training loss [negative]: 0.008670 | validation loss: 0.010246\n",
      "Epoch 54 | training loss [negative]: 0.008655 | validation loss: 0.010216\n",
      "Epoch 55 | training loss [negative]: 0.008634 | validation loss: 0.010793\n",
      "Epoch 56 | training loss [negative]: 0.008562 | validation loss: 0.011530\n",
      "Epoch 57 | training loss [negative]: 0.008533 | validation loss: 0.012592\n",
      "Epoch 58 | training loss [negative]: 0.008676 | validation loss: 0.011432\n",
      "Epoch 59 | training loss [negative]: 0.008583 | validation loss: 0.011312\n",
      "Epoch 60 | training loss [negative]: 0.008503 | validation loss: 0.010071\n",
      "Epoch 61 | training loss [negative]: 0.008602 | validation loss: 0.013708\n",
      "Epoch 62 | training loss [negative]: 0.008617 | validation loss: 0.010043\n",
      "Epoch 63 | training loss [negative]: 0.008600 | validation loss: 0.010478\n",
      "Epoch 64 | training loss [negative]: 0.008542 | validation loss: 0.010399\n",
      "Epoch 65 | training loss [negative]: 0.008633 | validation loss: 0.011371\n",
      "Epoch 66 | training loss [negative]: 0.008490 | validation loss: 0.010166\n",
      "Epoch 67 | training loss [negative]: 0.008446 | validation loss: 0.010270\n",
      "Epoch 68 | training loss [negative]: 0.008638 | validation loss: 0.012744\n",
      "Epoch 69 | training loss [negative]: 0.008481 | validation loss: 0.011040\n",
      "Epoch 70 | training loss [negative]: 0.008585 | validation loss: 0.010922\n",
      "Epoch 71 | training loss [negative]: 0.008358 | validation loss: 0.009736\n",
      "Epoch 72 | training loss [negative]: 0.008038 | validation loss: 0.013407\n",
      "Epoch 73 | training loss [negative]: 0.007415 | validation loss: 0.009037\n",
      "Epoch 74 | training loss [negative]: 0.007056 | validation loss: 0.009600\n",
      "Epoch 75 | training loss [negative]: 0.006760 | validation loss: 0.010042\n",
      "Epoch 76 | training loss [negative]: 0.006385 | validation loss: 0.007533\n",
      "Epoch 77 | training loss [negative]: 0.005839 | validation loss: 0.007442\n",
      "Epoch 78 | training loss [negative]: 0.005266 | validation loss: 0.006482\n",
      "Epoch 79 | training loss [negative]: 0.005030 | validation loss: 0.006395\n",
      "Epoch 80 | training loss [negative]: 0.005012 | validation loss: 0.006115\n",
      "Epoch 81 | training loss [negative]: 0.004940 | validation loss: 0.005323\n",
      "Epoch 82 | training loss [negative]: 0.004898 | validation loss: 0.005973\n",
      "Epoch 83 | training loss [negative]: 0.004927 | validation loss: 0.005711\n",
      "Epoch 84 | training loss [negative]: 0.004963 | validation loss: 0.007027\n",
      "Epoch 85 | training loss [negative]: 0.004874 | validation loss: 0.007795\n",
      "Epoch 86 | training loss [negative]: 0.004909 | validation loss: 0.006036\n",
      "Epoch 87 | training loss [negative]: 0.004885 | validation loss: 0.005764\n",
      "Epoch 88 | training loss [negative]: 0.004881 | validation loss: 0.006475\n",
      "Epoch 89 | training loss [negative]: 0.004878 | validation loss: 0.005993\n",
      "Epoch 90 | training loss [negative]: 0.004898 | validation loss: 0.005965\n",
      "Epoch 91 | training loss [negative]: 0.004883 | validation loss: 0.006867\n",
      "Epoch 92 | training loss [negative]: 0.004950 | validation loss: 0.006408\n",
      "Epoch 93 | training loss [negative]: 0.004882 | validation loss: 0.005865\n",
      "Epoch 94 | training loss [negative]: 0.004833 | validation loss: 0.005626\n",
      "Epoch 95 | training loss [negative]: 0.004845 | validation loss: 0.006054\n",
      "Epoch 96 | training loss [negative]: 0.004851 | validation loss: 0.006015\n",
      "Epoch 97 | training loss [negative]: 0.004895 | validation loss: 0.005333\n",
      "Epoch 98 | training loss [negative]: 0.004842 | validation loss: 0.005362\n",
      "Epoch 99 | training loss [negative]: 0.004845 | validation loss: 0.004839\n",
      "Epoch 100 | training loss [negative]: 0.004876 | validation loss: 0.004869\n"
     ]
    }
   ],
   "source": [
    "# TODO: training\n",
    "epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i + 1} | \", end=\"\")\n",
    "\n",
    "    # negative samples\n",
    "    # tr_losses = []\n",
    "    tr_loss_per_epoch = []\n",
    "    model.train()\n",
    "    for tr_x, _ in tr_neg_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        reconstructed_x = model(tr_x)\n",
    "\n",
    "        tr_loss = criterion(reconstructed_x, tr_x)\n",
    "        tr_loss_per_epoch.append(tr_loss.item())\n",
    "\n",
    "        tr_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_tr_mse_loss = sum(tr_loss_per_epoch) / len(tr_loss_per_epoch)\n",
    "    print(f'training loss [negative]: {avg_tr_mse_loss:2.6f} |', end=' ')\n",
    "    # tr_losses.append(avg_tr_mse_loss)\n",
    "\n",
    "    val_losses = []\n",
    "    val_loss_per_epoch = []\n",
    "    model.eval()\n",
    "    for val_x, _ in tt_loader:\n",
    "        reconstructed_x = model(val_x)\n",
    "\n",
    "        val_loss = criterion(reconstructed_x, val_x)\n",
    "        val_loss_per_epoch.append(val_loss.item())\n",
    "\n",
    "    avg_val_mse_loss = sum(val_loss_per_epoch) / len(val_loss_per_epoch)\n",
    "    print(f'validation loss: {avg_val_mse_loss:2.6f}')\n",
    "    val_losses.append(avg_val_mse_loss)\n",
    "\n",
    "    # if avg_val_mse_loss < best_val_loss:\n",
    "    #     best_val_loss = avg_val_mse_loss\n",
    "    # else:\n",
    "    #     print(f'Early Stopping')\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1713, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4428e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# TODO: check mse of positive training samples\n",
    "mse_of_pos_sample = []\n",
    "model.eval()\n",
    "for x_pos, y in tr_pos_loader:\n",
    "    reconstructed_x = model(x_pos)\n",
    "    mse_loss = criterion(reconstructed_x, x_pos)\n",
    "    mse_of_pos_sample.append(mse_loss)\n",
    "\n",
    "print(max(mse_of_pos_sample))\n",
    "print(min(mse_of_pos_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0043, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(mse_of_pos_sample[len(mse_of_pos_sample) // 2 + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test inference\n",
    "threshold = 0.002\n",
    "\n",
    "tt_loader = DataLoader(test, batch_size=1, shuffle=False, pin_memory=True, drop_last=False)\n",
    "model.eval()\n",
    "is_converted = []\n",
    "for x_tt, _ in tt_loader:\n",
    "    reconstructed_x = model(x_tt)\n",
    "    mse_loss = criterion(reconstructed_x, x_tt)\n",
    "\n",
    "    # negative sample만을 학습한 모델에 데이터를 입력했을 때, 설정한 threshold 값보다 높다면 -> positive sample\n",
    "    if mse_loss.item() >= threshold:\n",
    "        is_converted.append(True)\n",
    "\n",
    "    # threshold보다 낮다면 -> negative sample\n",
    "    else:\n",
    "        is_converted.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2140"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = np.array(is_converted)\n",
    "sum(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ood_detection_prototype_tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(dir_name='05_ood_detection',\n",
    "                y_pred=y_test_pred,\n",
    "                model_name=model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lgaimers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
