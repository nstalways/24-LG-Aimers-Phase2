{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import xgboost as xgb\n",
    "\n",
    "# sampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# custom modules\n",
    "from utils import set_seed, get_clf_eval, make_submission, record_experimental_results\n",
    "import preprocessing as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'seed': 33\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(hparams['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실험 01: `XGBClassifier()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb global configuration\n",
    "xgb.config_context(**{'verbosity': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb general parameters\n",
    "general_hparams = {\n",
    "    'booster': 'gbtree', # which booster to use. Can be gbtree, gblinear or dart; gbtree and dart use tree based models while gblinear uses linear functions.\n",
    "    'device': 'cuda', # Device for XGBoost to run.\n",
    "    'verbositiy': 2, # Verbosity of printing messages.\n",
    "    'validate_parameters': False, # When set to True, XGBoost will perform validation of input parameters to check whether a parameter is used or not.\n",
    "    # 'nthread': ??? # Number of parallel threads used to run XGBoost.\n",
    "    'disable_default_eval_metric': False, # Flag to disable default metric.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree hyperparameters\n",
    "treeBooster_hparams = {\n",
    "    'eta': 0.3, # Step size shrinkage used in update to prevents overfitting. (=learning rate)\n",
    "    'gamma': 0, # Minimum loss reduction required to make a further partition on a leaf node of the tree. (=min_split_loss)\n",
    "    'max_depth': 6, # Maximum depth of a tree.\n",
    "    'min_child_weight': 1, # Minimum sum of instance weight (hessian) needed in a child. (=min_pruning_val)\n",
    "    'max_delta_step': 0, # Maximum delta step we allow each leaf output to be. (???)\n",
    "    'subsample': 1, # Subsample ratio of the training instances\n",
    "    'sampling_method': 'uniform', # The method to use to sample the training instances.\n",
    "    'colsample_bytree': 1, # the subsample ratio of columns when constructing each tree. (=min_features)\n",
    "    'colsample_bylevel': 1, # the subsample ratio of columns for each level.\n",
    "    'colsample_bynode': 1, #  the subsample ratio of columns for each node (split).\n",
    "    'lambda': 1, # L2 regularization term on weights.\n",
    "    'alpha': 0, # L1 regularization term on weights.\n",
    "    'tree_method': 'auto', # The tree construction algorithm used in XGBoost.\n",
    "    'scale_pos_weight': 1, # Control the balance of positive and negative weights, useful for unbalanced classes.\n",
    "    # 'updater': ??, # A comma separated string defining the sequence of tree updaters to run, providing a modular way to construct and to modify the trees.\n",
    "    # 'refresh_leaf': 1, # This is a parameter of the refresh updater.\n",
    "    'process_type': 'default', # A type of boosting process to run. (=warmup)\n",
    "    'grow_policy': 'depthwise', # Controls a way new nodes are added to the tree.\n",
    "    'max_leaves': 0, # Maximum number of nodes to be added.\n",
    "    'max_bin': 256, # Only used if tree_method is set to hist or approx. (Maximum number of discrete bins to bucket continuous features.)\n",
    "    'num_parellel_tree': 1, # Number of parallel trees constructed during each iteration. (This option is used to support boosted random forest.)\n",
    "    # 'monotone_constraints': ??,\n",
    "    # 'interaction_constraints': ??,\n",
    "    'multi_strategy': 'one_output_per_tree', # The strategy used for training multi-target models, including multi-target regression and multi-class classification.\n",
    "    'max_cached_hist_node': 65536, # Maximum number of cached nodes for CPU histogram.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task hyperparameters\n",
    "task_hparams = {\n",
    "    'objective': 'binary:logistic', # Specify the learning task and the corresponding learning objective.\n",
    "    # 'base_score': , # The initial prediction score of all instances, global bias\n",
    "    # 'eval_metric': # Evaluation metrics for validation data, a default metric will be assigned according to objective\n",
    "    'seed': hparams['seed'], # Random number seed.\n",
    "    'seed_per_iteration': False, # Seed PRNG determnisticly via iterator number.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data, tt_data = pp.load_data()\n",
    "tr_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load & label encoding\n",
    "tr_data, tt_data = pp.load_data()\n",
    "x_tr, x_tt = pp.label_encoding(tr_data, tt_data)\n",
    "x_tr, y_tr, x_val, y_val = pp.split_train_and_validation(x_tr, val_size=0.1, seed=hparams['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=hparams['seed'])\n",
    "x_val, y_val = rus.fit_resample(x_val, y_val)\n",
    "len(x_val), len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data load & label encoding\n",
    "# tr_data, tt_data = pp.load_data()\n",
    "# x_tr, x_tt = pp.label_encoding(tr_data, tt_data)\n",
    "\n",
    "# # 서로 다른 seed를 이용하여 undersampling 수행\n",
    "# rus = RandomUnderSampler(random_state=hparams['seed'])\n",
    "# x_tr_res, y_tr_res = rus.fit_resample(x_tr.drop(['is_converted'], axis=1), x_tr['is_converted'])\n",
    "\n",
    "# # train / validation split\n",
    "# x_tr_res['is_converted'] = y_tr_res # concat\n",
    "# x_tr, y_tr, x_val, y_val = pp.split_train_and_validation(x_tr_res, seed=hparams['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treeBooster_hparams_ablation = treeBooster_hparams.copy()\n",
    "\n",
    "treeBooster_hparams_ablation['scale_pos_weight'] = len(tr_data[tr_data['is_converted'] == False]) / len(tr_data[tr_data['is_converted'] == True])\n",
    "treeBooster_hparams_ablation['max_depth'] = None\n",
    "treeBooster_hparams_ablation['lambda'] = 10\n",
    "treeBooster_hparams_ablation['eta'] = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc = xgb.XGBClassifier(**general_hparams, **treeBooster_hparams_ablation, **task_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc.fit(x_tr.fillna(0), y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check validation score\n",
    "y_val_pred = xgbc.predict(x_val.fillna(0))\n",
    "get_clf_eval(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# x_tt = x_tt.drop(['is_converted', 'id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = xgbc.predict(x_tt.fillna(0))\n",
    "sum(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'baseline_xgb_decEta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(dir_name='06_xgboost',\n",
    "                y_pred=y_test_pred,\n",
    "                model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Record**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_experimental_results(model_name=model_name,\n",
    "                            test_f1_score='0.6844741235392321',\n",
    "                            description='xgboost baseline에서 eta를 0.05로 줄임')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lgaimers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
