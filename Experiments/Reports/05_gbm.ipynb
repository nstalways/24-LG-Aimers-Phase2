{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[None]</span> 실험 01: `GradientBoostingClassifier()`**\n",
    "\n",
    "**Motivation**\n",
    "- AdaBoost 실험을 통해 estimator의 capcity가 모델 성능에 있어 중요할 것으로 판단하였고, 그에 따라 Gradient Boosting Models (이하 GBM) 실험을 진행하게 되었다.\n",
    "<br><br>\n",
    "\n",
    "**Baseline parameters**\n",
    "```python\n",
    "gbm_hparams = {\n",
    "    'loss': 'log_loss', # The loss function to be optimized.\n",
    "    'learning_rate':0.1, # Learning rate shrinks the contribution of each tree by learning_rate. \n",
    "    'n_estimators': 100, # The number of boosting stages to perform.\n",
    "    'subsample': 1.0, # The fraction of samples to be used for fitting the individual base learners.\n",
    "    'criterion': 'friedman_mse', # The function to measure the quality of a split.\n",
    "    'min_samples_split': 2, # The minimum number of samples required to split an internal node:\n",
    "    'min_samples_leaf': 1, # The minimum number of samples required to be at a leaf node.\n",
    "    'max_depth': 3, # Maximum depth of the individual regression estimators.\n",
    "    'min_impurity_decrease': 0.0, # A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "    'init': None, # An estimator object that is used to compute the initial predictions.\n",
    "    # 'random_state': hparams['seed'], # Controls the random seed given to each Tree estimator at each boosting iteration.\n",
    "    'max_features': None, # The number of features to consider when looking for the best split:\n",
    "    'verbose': 0, # Enable verbose output.\n",
    "    'max_leaf_nodes': None, # Grow trees with max_leaf_nodes in best-first fashion.\n",
    "    'warm_start': False,\n",
    "    'validation_fraction': 0.1, # The proportion of training data to set aside as validation set for early stopping.\n",
    "    'n_iter_no_change': None, # n_iter_no_change is used to decide if early stopping will be used to terminate training when validation score is not improving.\n",
    "    'tol': 1e-4, # Tolerance for the early stopping.\n",
    "    'ccp_alpha': 0.0 # Complexity parameter used for Minimal Cost-Complexity Pruning.\n",
    "}\n",
    "```\n",
    "<br>\n",
    "\n",
    "**Details**\n",
    "- 원본 데이터를 그대로 사용하여 모델을 학습했다. (Sampler 사용 X)\n",
    "- 학습한 모델을 테스트해보았을 때 성능이 너무 떨어지는 관계로, validation set에 대한 score를 기준으로 hyperparameters에 따른 성능 차이를 비교분석했다.\n",
    "<br><br>\n",
    "\n",
    "**Validation f1 score**\n",
    "- `Table 1`: `n_estimators` 값 변경에 따른 실험 결과\n",
    "\n",
    "    |n_estimators|precision|recall|f1 score|\n",
    "    |:-:|:-:|:-:|:-:|\n",
    "    | 100 | 0.9585 | 0.5867 | 0.7279 |\n",
    "    | 200 | 0.9535 | 0.6279 | 0.7572 |\n",
    "    | 400 | 0.9530 | 0.6638 | 0.7826 |\n",
    "    | 800 | 0.9268 | 0.7093 | 0.8036 |\n",
    "<br>\n",
    "\n",
    "- `Table 2`: `learning_rate` 와 `n_estimators` 값에 따른 실험 결과\n",
    "\n",
    "    |learning_rate|n_estimators|precision|recall|f1 score|\n",
    "    |:-:|:-:|:-:|:-:|:-:|\n",
    "    | 0.1 | 100 | 0.9585 | 0.5867 | 0.7279 |\n",
    "    | 0.2 | 100 | 0.9422 | 0.6374 | 0.7604 |\n",
    "    | 0.4 | 100 | 0.9272 | 0.6998 | 0.7976 |\n",
    "    | 0.1 | 200 | 0.9535 | 0.6279 | 0.7572 |\n",
    "    | 0.2 | 200 | 0.9441 | 0.6786 | 0.7897 |\n",
    "    | 0.4 | 200 | 0.9205 | 0.7347 | 0.8172 |\n",
    "    | 0.1 | 400 | 0.9530 | 0.6638 | 0.7826 |\n",
    "    | 0.2 | 400 | 0.9235 | 0.7146 | 0.8057 |\n",
    "    | 0.4 | 400 | 0.9011 | 0.7611 | 0.8252 |\n",
    "<br>\n",
    "\n",
    "- `Table 3`: **Undersampling을 통해 positive, negative samples의 비율을 동일하게 맞춰준 상태**에서, <br>`learning_rate` 와 `n_estimators` 값에 따른 실험 결과\n",
    "\n",
    "    |learning_rate|n_estimators|precision|recall|f1 score|\n",
    "    |:-:|:-:|:-:|:-:|:-:|\n",
    "    | 0.1 | 100 | 0.9107 | 0.9079 | 0.9093 |\n",
    "    | 0.2 | 100 | 0.9213 | 0.9213 | 0.9213 |\n",
    "    | 0.4 | 100 | 0.9236 | 0.9265 | 0.9251 |\n",
    "    | 0.1 | 200 | 0.9139 | 0.9234 | 0.9186 |\n",
    "    | 0.2 | 200 | 0.9253 | 0.9358 | 0.9305 |\n",
    "    | 0.4 | 200 | 0.9176 | 0.9337 | 0.9256 |\n",
    "    | 0.1 | 400 | 0.9213 | 0.9337 | 0.9275 |\n",
    "    | 0.2 | 400 | 0.9265 | 0.9389 | 0.9326 |\n",
    "    | 0.4 | 400 | 0.9261 | 0.9472 | 0.9365 |\n",
    "<br>\n",
    "\n",
    "- `Table 4`: `max_depth` 값 변경에 따른 실험 결과\n",
    "\n",
    "    |max_depth|precision|recall|f1 score|\n",
    "    |:-:|:-:|:-:|:-:|\n",
    "    | 3 | 0.9585 | 0.5867 | 0.7279 |\n",
    "    | 6 | 0.9408 | 0.6882 | 0.7949 |\n",
    "    | 9 | 0.9186 | 0.7516 | 0.8267 |\n",
    "    | 12 | 0.9333 | 0.7696 | 0.8436 |\n",
    "    | None | 0.7943 | 0.7960 | 0.7951 |\n",
    "<br>    \n",
    "\n",
    "**Analysis**\n",
    "- `Table 1`을 보면, `n_estimators`를 증가시킬수록 f1 score가 높아지는 것을 볼 수 있다. 정답과 예측 간의 residual을 줄이도록 모델링되는 gbm의 특성 상, estimators의 개수가 증가함에 따라 보다 세밀한 데이터 모델링이 가능해지기 때문에 f1 score 또한 증가한 것으로 분석하였다.\n",
    "<br><br>\n",
    "- `Table 2`와 `Table 3`은 `learning_rate`와 `n_estimators`의 관계를 나타내는 실험 결과이다. 일반적으로 `n_estimators` 값이 커질수록 `learning_rate`를 줄이는 것이 성능에 유리하다고 하지만, 실제로 실험해보았을 때에는 둘 다 키웠음에도 validation f1 score가 높아지는 것을 확인할 수 있었다.\n",
    "- `Table 2`는 아무런 sampling 없이, imbalanced dataset을 그대로 학습한 gbm의 성능을 나타내고 있다. 여기서는 `learning_rate`가 커짐에 따라 precision은 낮아지고, recall은 높아지고 있다. 아무래도 imbalanced dataset의 절대 다수를 차지하고 있는 negative samples로 인해서 residuals이 positive samples에서 많이 발생할 것이고, 이를 보완하기 위해 estimators가 학습되었을 것이며, 큰 `learning_rate`로 인해 해당 estimators의 예측값이 많이 반영되면서 나타난 결과로 분석하였다.\n",
    "- Undersampling을 통해 balanced dataset을 만든 뒤 학습시킨 gbm의 성능을 나타내고 있는 `Table 3`은 결과 양상이 조금 달랐다. Precision과 recall 모두 향상되는 모습이었다. `Table 3`을 통해 default 값인 `learning_rate = 0.1`이 너무 낮은 값이었을 수도 있겠다고 판단하였다.\n",
    "<br><br>\n",
    "- `Table 4`를 통해 gbm의 성능에 가장 크게 영향을 미치는 hyperparameters는 `max_depth`라는 것을 알 수 있었다. `max_depth` 값을 키울수록 validation f1 score 또한 높아지는 선형적인 관계가 보였다. 다만 `max_depth`를 설정하지 않는 경우 training data에 overfitting이 발생하여 성능이 큰 폭으로 하락하였고, 이를 방지하고자 `ccp_alpha`를 적용하는 경우 더 큰 성능 하락을 보였다.\n",
    "\n",
    "**정리**\n",
    "\n",
    "`+`\n",
    "- increase n_estimators\n",
    "- increase learning_rate\n",
    "- increase max_depth\n",
    "---\n",
    "`-`\n",
    "- cost-complexity pruning\n",
    "\n",
    "**Future works**\n",
    "- ensemble\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:orange\">[0.743392, Best]</span></span> 실험 02: `GradientBoostingClassifier()` ensemble**\n",
    "\n",
    "**Motivation**\n",
    "- 다른 모델에 비해 GBM이 imbalanced data 대처 능력이 높긴 하지만, 그럼에도 불구하고 negative samples에 편향된 예측 결과를 보이고 있다.\n",
    "- 따라서 undersampling을 통해 subset을 만든 뒤, 각각의 subset에 대해 gbm을 학습시키고, 이를 ensemble하는 전략을 시도하였다.\n",
    "<br><br>\n",
    "\n",
    "**Baseline parameters**\n",
    "```python\n",
    "gbm_hparams = {\n",
    "    'loss': 'log_loss', # The loss function to be optimized.\n",
    "    'learning_rate':0.1, # Learning rate shrinks the contribution of each tree by learning_rate. \n",
    "    'n_estimators': 100, # The number of boosting stages to perform.\n",
    "    'subsample': 1.0, # The fraction of samples to be used for fitting the individual base learners.\n",
    "    'criterion': 'friedman_mse', # The function to measure the quality of a split.\n",
    "    'min_samples_split': 2, # The minimum number of samples required to split an internal node:\n",
    "    'min_samples_leaf': 1, # The minimum number of samples required to be at a leaf node.\n",
    "    'max_depth': 3, # Maximum depth of the individual regression estimators.\n",
    "    'min_impurity_decrease': 0.0, # A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "    'init': None, # An estimator object that is used to compute the initial predictions.\n",
    "    # 'random_state': hparams['seed'], # Controls the random seed given to each Tree estimator at each boosting iteration.\n",
    "    'max_features': None, # The number of features to consider when looking for the best split:\n",
    "    'verbose': 0, # Enable verbose output.\n",
    "    'max_leaf_nodes': None, # Grow trees with max_leaf_nodes in best-first fashion.\n",
    "    'warm_start': False,\n",
    "    'validation_fraction': 0.1, # The proportion of training data to set aside as validation set for early stopping.\n",
    "    'n_iter_no_change': None, # n_iter_no_change is used to decide if early stopping will be used to terminate training when validation score is not improving.\n",
    "    'tol': 1e-4, # Tolerance for the early stopping.\n",
    "    'ccp_alpha': 0.0 # Complexity parameter used for Minimal Cost-Complexity Pruning.\n",
    "}\n",
    "```\n",
    "<br>\n",
    "\n",
    "**Details**\n",
    "- Baseline parameters 중 성능에 영향을 미치는 parameter 일부를 변경해가면서 실험을 진행하였다.\n",
    "<br><br>\n",
    "\n",
    "**Test f1 score**\n",
    "- Previous best test f1 score: 0.717894\n",
    "- `hparams01` == Baseline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>model_name</th>\n",
       "      <th>test_f1_score</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2024-02-24/00:12:24</td>\n",
       "      <td>hparams05_gbm_30_inc_estimators_dropDuplicates</td>\n",
       "      <td>0.743392</td>\n",
       "      <td>hparams05 세팅 + 중복 데이터 (negative 3000개, positive 200개) 삭제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2024-02-23/13:58:11</td>\n",
       "      <td>hparams05_gbm_30_inc_estimators</td>\n",
       "      <td>0.737768</td>\n",
       "      <td>hparams04 세팅에서 n_estimators를 400으로 증가시킨 뒤에, gbm 30개를 학습시키고 ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2024-02-23/13:12:49</td>\n",
       "      <td>hparams04_gbm_20_inc_estimators</td>\n",
       "      <td>0.730919</td>\n",
       "      <td>hparams03 세팅에서 n_estimators를 200으로 증가시킨 뒤에, gbm 20개를 학습시키고 ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2024-02-23/00:25:20</td>\n",
       "      <td>hparams03_gbm_20_incMaxDepth</td>\n",
       "      <td>0.726567</td>\n",
       "      <td>hparams01 세팅에서 max_depth를 6으로 증가시킨 뒤에, gbm 20개를 학습시키고 ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2024-02-22/23:34:11</td>\n",
       "      <td>hparams01_gbm_20_noSplit</td>\n",
       "      <td>0.710039</td>\n",
       "      <td>Data split을 하지 않은 상태에서, hparams01 세팅으로 gbm 20개를 학습시키고 ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2024-02-22/23:14:07</td>\n",
       "      <td>hparams01_gbm_20</td>\n",
       "      <td>0.704826</td>\n",
       "      <td>random_state를 제외한 hparams을 default 값으로 사용한 gbm 20개를 학습시킨 뒤 ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2024-02-22/23:45:26</td>\n",
       "      <td>hparams01_gbm_100_noSplit</td>\n",
       "      <td>0.704304</td>\n",
       "      <td>Data split을 하지 않은 상태에서, hparams01 세팅으로 gbm 100개를 학습시키고 ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2024-02-22/23:20:44</td>\n",
       "      <td>hparams02_gbm_20</td>\n",
       "      <td>0.698675</td>\n",
       "      <td>hparams01에서 ccp_alpha를 0.0004로 변경한 뒤 gbm 20개를 학습시키고 ensemble</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   date                                      model_name  \\\n",
       "45  2024-02-24/00:12:24  hparams05_gbm_30_inc_estimators_dropDuplicates   \n",
       "42  2024-02-23/13:58:11                 hparams05_gbm_30_inc_estimators   \n",
       "41  2024-02-23/13:12:49                 hparams04_gbm_20_inc_estimators   \n",
       "40  2024-02-23/00:25:20                    hparams03_gbm_20_incMaxDepth   \n",
       "38  2024-02-22/23:34:11                        hparams01_gbm_20_noSplit   \n",
       "36  2024-02-22/23:14:07                                hparams01_gbm_20   \n",
       "39  2024-02-22/23:45:26                       hparams01_gbm_100_noSplit   \n",
       "37  2024-02-22/23:20:44                                hparams02_gbm_20   \n",
       "\n",
       "    test_f1_score  \\\n",
       "45       0.743392   \n",
       "42       0.737768   \n",
       "41       0.730919   \n",
       "40       0.726567   \n",
       "38       0.710039   \n",
       "36       0.704826   \n",
       "39       0.704304   \n",
       "37       0.698675   \n",
       "\n",
       "                                                            description  \n",
       "45             hparams05 세팅 + 중복 데이터 (negative 3000개, positive 200개) 삭제  \n",
       "42  hparams04 세팅에서 n_estimators를 400으로 증가시킨 뒤에, gbm 30개를 학습시키고 ensemble  \n",
       "41  hparams03 세팅에서 n_estimators를 200으로 증가시킨 뒤에, gbm 20개를 학습시키고 ensemble  \n",
       "40       hparams01 세팅에서 max_depth를 6으로 증가시킨 뒤에, gbm 20개를 학습시키고 ensemble  \n",
       "38       Data split을 하지 않은 상태에서, hparams01 세팅으로 gbm 20개를 학습시키고 ensemble  \n",
       "36  random_state를 제외한 hparams을 default 값으로 사용한 gbm 20개를 학습시킨 뒤 ensemble  \n",
       "39      Data split을 하지 않은 상태에서, hparams01 세팅으로 gbm 100개를 학습시키고 ensemble  \n",
       "37         hparams01에서 ccp_alpha를 0.0004로 변경한 뒤 gbm 20개를 학습시키고 ensemble  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.read_csv(\"../experimental_records.csv\")\n",
    "df = df[df['model_name'].apply(lambda x: True if x.find('gbm') + 1 else False)]\n",
    "df.sort_values(by='test_f1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "- `max_depth`와 `n_estimators` 값을 키웠을 때 부터 test f1 score가 높아지기 시작했다.\n",
    "- 가장 성능이 좋은 모델인 `hparams05_gbm_30_inc_estimators`는 `n_estimators == 400`, `max_depth == 6` 인 gbm 30개를 ensemble한 결과였다.\n",
    "- 다양한 hparams 조합을 실험해본 결과, 실험 01과는 조금 다른 양상을 확인할 수 있었다.\n",
    "    - `learning_rate`는 0.1일 때 가장 좋은 성능을 보였다. (그 이상인 경우 validation score 감소)\n",
    "    - `max_depth`는 6일 때 가장 좋은 성능을 보였다. (그 이상인 경우 validation score 감소)\n",
    "    - `n_estimators`는 값을 키울수록 성능이 좋아졌다. 다만 일정 개수 이상부터는 성능 변화가 거의 없었다 (하락은 하지 않았다).\n",
    "- Ensemble할 모델의 개수와 `n_estimators` 값을 더 높인다면 성능이 올라갈 것 같기는 하지만, 크게 향상될 것 같다는 생각은 들지 않아서 실험을 마무리하였다.\n",
    "- 학습 데이터에 중복된 데이터가 존재할 수 있다는 점을 망각하여 전처리를 하지 않았었는데, 중복 데이터를 삭제한 뒤 hparams05 세팅으로 실험한 결과 test f1 score가 크게 향상되었음.\n",
    "\n",
    "**정리**\n",
    "\n",
    "`+`\n",
    "- increase n_estimators\n",
    "- increase max_depth\n",
    "---\n",
    "`-`\n",
    "- increase learning_rate\n",
    "\n",
    "**Future works**\n",
    "- xgboost, LightGBM, CatBoost\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lgaimers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
