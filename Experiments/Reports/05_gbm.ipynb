{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[None]</span> 실험 01: `GradientBoostingClassifier()`**\n",
    "\n",
    "**Motivation**\n",
    "- AdaBoost 실험을 통해 estimator의 capcity가 모델 성능에 있어 중요할 것으로 판단하였고, 그에 따라 Gradient Boosting Models (이하 GBM) 실험을 진행하게 되었다.\n",
    "<br><br>\n",
    "\n",
    "**Baseline parameters**\n",
    "```python\n",
    "gbm_hparams = {\n",
    "    'loss': 'log_loss', # The loss function to be optimized.\n",
    "    'learning_rate':0.1, # Learning rate shrinks the contribution of each tree by learning_rate. \n",
    "    'n_estimators': 100, # The number of boosting stages to perform.\n",
    "    'subsample': 1.0, # The fraction of samples to be used for fitting the individual base learners.\n",
    "    'criterion': 'friedman_mse', # The function to measure the quality of a split.\n",
    "    'min_samples_split': 2, # The minimum number of samples required to split an internal node:\n",
    "    'min_samples_leaf': 1, # The minimum number of samples required to be at a leaf node.\n",
    "    'max_depth': 3, # Maximum depth of the individual regression estimators.\n",
    "    'min_impurity_decrease': 0.0, # A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "    'init': None, # An estimator object that is used to compute the initial predictions.\n",
    "    # 'random_state': hparams['seed'], # Controls the random seed given to each Tree estimator at each boosting iteration.\n",
    "    'max_features': None, # The number of features to consider when looking for the best split:\n",
    "    'verbose': 0, # Enable verbose output.\n",
    "    'max_leaf_nodes': None, # Grow trees with max_leaf_nodes in best-first fashion.\n",
    "    'warm_start': False,\n",
    "    'validation_fraction': 0.1, # The proportion of training data to set aside as validation set for early stopping.\n",
    "    'n_iter_no_change': None, # n_iter_no_change is used to decide if early stopping will be used to terminate training when validation score is not improving.\n",
    "    'tol': 1e-4, # Tolerance for the early stopping.\n",
    "    'ccp_alpha': 0.0 # Complexity parameter used for Minimal Cost-Complexity Pruning.\n",
    "}\n",
    "```\n",
    "<br>\n",
    "\n",
    "**Details**\n",
    "- 원본 데이터를 그대로 사용하여 모델을 학습했다. (Sampler 사용 X)\n",
    "- 학습한 모델을 테스트해보았을 때 성능이 너무 떨어지는 관계로, validation set에 대한 score를 기준으로 hyperparameters에 따른 성능 차이를 비교분석했다.\n",
    "<br><br>\n",
    "\n",
    "**Validation f1 score**\n",
    "- `Table 1`: `n_estimators` 값 변경에 따른 실험 결과\n",
    "\n",
    "    |n_estimators|precision|recall|f1 score|\n",
    "    |:-:|:-:|:-:|:-:|\n",
    "    | 100 | 0.9585 | 0.5867 | 0.7279 |\n",
    "    | 200 | 0.9535 | 0.6279 | 0.7572 |\n",
    "    | 400 | 0.9530 | 0.6638 | 0.7826 |\n",
    "    | 800 | 0.9268 | 0.7093 | 0.8036 |\n",
    "<br>\n",
    "\n",
    "- `Table 2`: `learning_rate` 와 `n_estimators` 값에 따른 실험 결과\n",
    "\n",
    "    |learning_rate|n_estimators|precision|recall|f1 score|\n",
    "    |:-:|:-:|:-:|:-:|:-:|\n",
    "    | 0.1 | 100 | 0.9585 | 0.5867 | 0.7279 |\n",
    "    | 0.2 | 100 | 0.9422 | 0.6374 | 0.7604 |\n",
    "    | 0.4 | 100 | 0.9272 | 0.6998 | 0.7976 |\n",
    "    | 0.1 | 200 | 0.9535 | 0.6279 | 0.7572 |\n",
    "    | 0.2 | 200 | 0.9441 | 0.6786 | 0.7897 |\n",
    "    | 0.4 | 200 | 0.9205 | 0.7347 | 0.8172 |\n",
    "    | 0.1 | 400 | 0.9530 | 0.6638 | 0.7826 |\n",
    "    | 0.2 | 400 | 0.9235 | 0.7146 | 0.8057 |\n",
    "    | 0.4 | 400 | 0.9011 | 0.7611 | 0.8252 |\n",
    "<br>\n",
    "\n",
    "- `Table 3`: **Undersampling을 통해 positive, negative samples의 비율을 동일하게 맞춰준 상태**에서, <br>`learning_rate` 와 `n_estimators` 값에 따른 실험 결과\n",
    "\n",
    "    |learning_rate|n_estimators|precision|recall|f1 score|\n",
    "    |:-:|:-:|:-:|:-:|:-:|\n",
    "    | 0.1 | 100 | 0.9107 | 0.9079 | 0.9093 |\n",
    "    | 0.2 | 100 | 0.9213 | 0.9213 | 0.9213 |\n",
    "    | 0.4 | 100 | 0.9236 | 0.9265 | 0.9251 |\n",
    "    | 0.1 | 200 | 0.9139 | 0.9234 | 0.9186 |\n",
    "    | 0.2 | 200 | 0.9253 | 0.9358 | 0.9305 |\n",
    "    | 0.4 | 200 | 0.9176 | 0.9337 | 0.9256 |\n",
    "    | 0.1 | 400 | 0.9213 | 0.9337 | 0.9275 |\n",
    "    | 0.2 | 400 | 0.9265 | 0.9389 | 0.9326 |\n",
    "    | 0.4 | 400 | 0.9261 | 0.9472 | 0.9365 |\n",
    "<br>\n",
    "\n",
    "- `Table 4`: `max_depth` 값 변경에 따른 실험 결과\n",
    "\n",
    "    |max_depth|precision|recall|f1 score|\n",
    "    |:-:|:-:|:-:|:-:|\n",
    "    | 3 | 0.9585 | 0.5867 | 0.7279 |\n",
    "    | 6 | 0.9408 | 0.6882 | 0.7949 |\n",
    "    | 9 | 0.9186 | 0.7516 | 0.8267 |\n",
    "    | 12 | 0.9333 | 0.7696 | 0.8436 |\n",
    "    | None | 0.7943 | 0.7960 | 0.7951 |\n",
    "<br>    \n",
    "\n",
    "**Analysis**\n",
    "- `Table 1`을 보면, `n_estimators`를 증가시킬수록 f1 score가 높아지는 것을 볼 수 있다. 정답과 예측 간의 residual을 줄이도록 모델링되는 gbm의 특성 상, estimators의 개수가 증가함에 따라 보다 세밀한 데이터 모델링이 가능해지기 때문에 f1 score 또한 증가한 것으로 분석하였다.\n",
    "<br><br>\n",
    "- `Table 2`와 `Table 3`은 `learning_rate`와 `n_estimators`의 관계를 나타내는 실험 결과이다. 일반적으로 `n_estimators` 값이 커질수록 `learning_rate`를 줄이는 것이 성능에 유리하다고 하지만, 실제로 실험해보았을 때에는 둘 다 키웠음에도 validation f1 score가 높아지는 것을 확인할 수 있었다.\n",
    "- `Table 2`는 아무런 sampling 없이, imbalanced dataset을 그대로 학습한 gbm의 성능을 나타내고 있다. 여기서는 `learning_rate`가 커짐에 따라 precision은 낮아지고, recall은 높아지고 있다. 아무래도 imbalanced dataset의 절대 다수를 차지하고 있는 negative samples로 인해서 residuals이 positive samples에서 많이 발생할 것이고, 이를 보완하기 위해 estimators가 학습되었을 것이며, 큰 `learning_rate`로 인해 해당 estimators의 예측값이 많이 반영되면서 나타난 결과로 분석하였다.\n",
    "- Undersampling을 통해 balanced dataset을 만든 뒤 학습시킨 gbm의 성능을 나타내고 있는 `Table 3`은 결과 양상이 조금 달랐다. Precision과 recall 모두 향상되는 모습이었다. `Table 3`을 통해 default 값인 `learning_rate = 0.1`이 너무 낮은 값이었을 수도 있겠다고 판단하였다.\n",
    "<br><br>\n",
    "- `Table 4`를 통해 gbm의 성능에 가장 크게 영향을 미치는 hyperparameters는 `max_depth`라는 것을 알 수 있었다. `max_depth` 값을 키울수록 validation f1 score 또한 높아지는 선형적인 관계가 보였다. 다만 `max_depth`를 설정하지 않는 경우 training data에 overfitting이 발생하여 성능이 큰 폭으로 하락하였고, 이를 방지하고자 `ccp_alpha`를 적용하는 경우 더 큰 성능 하락을 보였다.\n",
    "\n",
    "**정리**\n",
    "\n",
    "`+`\n",
    "- increase n_estimators\n",
    "- increase learning_rate\n",
    "- increase max_depth\n",
    "---\n",
    "`-`\n",
    "- cost-complexity pruning\n",
    "\n",
    "**Future works**\n",
    "- ensemble\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">[0.743392]</span></span> 실험 02: `GradientBoostingClassifier()` ensemble**\n",
    "\n",
    "**Motivation**\n",
    "- 다른 모델에 비해 GBM이 imbalanced data 대처 능력이 높긴 하지만, 그럼에도 불구하고 negative samples에 편향된 예측 결과를 보이고 있다.\n",
    "- 따라서 undersampling을 통해 subset을 만든 뒤, 각각의 subset에 대해 gbm을 학습시키고, 이를 ensemble하는 전략을 시도하였다.\n",
    "<br><br>\n",
    "\n",
    "**Baseline parameters**\n",
    "```python\n",
    "gbm_hparams = {\n",
    "    'loss': 'log_loss', # The loss function to be optimized.\n",
    "    'learning_rate':0.1, # Learning rate shrinks the contribution of each tree by learning_rate. \n",
    "    'n_estimators': 100, # The number of boosting stages to perform.\n",
    "    'subsample': 1.0, # The fraction of samples to be used for fitting the individual base learners.\n",
    "    'criterion': 'friedman_mse', # The function to measure the quality of a split.\n",
    "    'min_samples_split': 2, # The minimum number of samples required to split an internal node:\n",
    "    'min_samples_leaf': 1, # The minimum number of samples required to be at a leaf node.\n",
    "    'max_depth': 3, # Maximum depth of the individual regression estimators.\n",
    "    'min_impurity_decrease': 0.0, # A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "    'init': None, # An estimator object that is used to compute the initial predictions.\n",
    "    # 'random_state': hparams['seed'], # Controls the random seed given to each Tree estimator at each boosting iteration.\n",
    "    'max_features': None, # The number of features to consider when looking for the best split:\n",
    "    'verbose': 0, # Enable verbose output.\n",
    "    'max_leaf_nodes': None, # Grow trees with max_leaf_nodes in best-first fashion.\n",
    "    'warm_start': False,\n",
    "    'validation_fraction': 0.1, # The proportion of training data to set aside as validation set for early stopping.\n",
    "    'n_iter_no_change': None, # n_iter_no_change is used to decide if early stopping will be used to terminate training when validation score is not improving.\n",
    "    'tol': 1e-4, # Tolerance for the early stopping.\n",
    "    'ccp_alpha': 0.0 # Complexity parameter used for Minimal Cost-Complexity Pruning.\n",
    "}\n",
    "```\n",
    "<br>\n",
    "\n",
    "**Details**\n",
    "- Baseline parameters 중 성능에 영향을 미치는 parameter 일부를 변경해가면서 실험을 진행하였다.\n",
    "<br><br>\n",
    "\n",
    "**Test f1 score**\n",
    "- Previous best test f1 score: 0.717894\n",
    "- `hparams01` == Baseline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>model_name</th>\n",
       "      <th>test_f1_score</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2024-02-24/00:12:24</td>\n",
       "      <td>hparams05_gbm_30_inc_estimators_dropDuplicates</td>\n",
       "      <td>0.743392</td>\n",
       "      <td>hparams05 세팅 + 중복 데이터 (negative 3000개, positive 200개) 삭제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2024-02-23/13:58:11</td>\n",
       "      <td>hparams05_gbm_30_inc_estimators</td>\n",
       "      <td>0.737768</td>\n",
       "      <td>hparams04 세팅에서 n_estimators를 400으로 증가시킨 뒤에, gbm 30개를 학습시키고 ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2024-02-23/13:12:49</td>\n",
       "      <td>hparams04_gbm_20_inc_estimators</td>\n",
       "      <td>0.730919</td>\n",
       "      <td>hparams03 세팅에서 n_estimators를 200으로 증가시킨 뒤에, gbm 20개를 학습시키고 ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2024-02-23/00:25:20</td>\n",
       "      <td>hparams03_gbm_20_incMaxDepth</td>\n",
       "      <td>0.726567</td>\n",
       "      <td>hparams01 세팅에서 max_depth를 6으로 증가시킨 뒤에, gbm 20개를 학습시키고 ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2024-02-22/23:34:11</td>\n",
       "      <td>hparams01_gbm_20_noSplit</td>\n",
       "      <td>0.710039</td>\n",
       "      <td>Data split을 하지 않은 상태에서, hparams01 세팅으로 gbm 20개를 학습시키고 ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2024-02-22/23:14:07</td>\n",
       "      <td>hparams01_gbm_20</td>\n",
       "      <td>0.704826</td>\n",
       "      <td>random_state를 제외한 hparams을 default 값으로 사용한 gbm 20개를 학습시킨 뒤 ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2024-02-22/23:45:26</td>\n",
       "      <td>hparams01_gbm_100_noSplit</td>\n",
       "      <td>0.704304</td>\n",
       "      <td>Data split을 하지 않은 상태에서, hparams01 세팅으로 gbm 100개를 학습시키고 ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2024-02-22/23:20:44</td>\n",
       "      <td>hparams02_gbm_20</td>\n",
       "      <td>0.698675</td>\n",
       "      <td>hparams01에서 ccp_alpha를 0.0004로 변경한 뒤 gbm 20개를 학습시키고 ensemble</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   date                                      model_name  \\\n",
       "45  2024-02-24/00:12:24  hparams05_gbm_30_inc_estimators_dropDuplicates   \n",
       "42  2024-02-23/13:58:11                 hparams05_gbm_30_inc_estimators   \n",
       "41  2024-02-23/13:12:49                 hparams04_gbm_20_inc_estimators   \n",
       "40  2024-02-23/00:25:20                    hparams03_gbm_20_incMaxDepth   \n",
       "38  2024-02-22/23:34:11                        hparams01_gbm_20_noSplit   \n",
       "36  2024-02-22/23:14:07                                hparams01_gbm_20   \n",
       "39  2024-02-22/23:45:26                       hparams01_gbm_100_noSplit   \n",
       "37  2024-02-22/23:20:44                                hparams02_gbm_20   \n",
       "\n",
       "    test_f1_score  \\\n",
       "45       0.743392   \n",
       "42       0.737768   \n",
       "41       0.730919   \n",
       "40       0.726567   \n",
       "38       0.710039   \n",
       "36       0.704826   \n",
       "39       0.704304   \n",
       "37       0.698675   \n",
       "\n",
       "                                                            description  \n",
       "45             hparams05 세팅 + 중복 데이터 (negative 3000개, positive 200개) 삭제  \n",
       "42  hparams04 세팅에서 n_estimators를 400으로 증가시킨 뒤에, gbm 30개를 학습시키고 ensemble  \n",
       "41  hparams03 세팅에서 n_estimators를 200으로 증가시킨 뒤에, gbm 20개를 학습시키고 ensemble  \n",
       "40       hparams01 세팅에서 max_depth를 6으로 증가시킨 뒤에, gbm 20개를 학습시키고 ensemble  \n",
       "38       Data split을 하지 않은 상태에서, hparams01 세팅으로 gbm 20개를 학습시키고 ensemble  \n",
       "36  random_state를 제외한 hparams을 default 값으로 사용한 gbm 20개를 학습시킨 뒤 ensemble  \n",
       "39      Data split을 하지 않은 상태에서, hparams01 세팅으로 gbm 100개를 학습시키고 ensemble  \n",
       "37         hparams01에서 ccp_alpha를 0.0004로 변경한 뒤 gbm 20개를 학습시키고 ensemble  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.read_csv(\"../experimental_records.csv\")\n",
    "df = df[df['model_name'].apply(lambda x: True if x.find('gbm') + 1 else False)].loc[:45]\n",
    "df.sort_values(by='test_f1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "- `max_depth`와 `n_estimators` 값을 키웠을 때 부터 test f1 score가 높아지기 시작했다.\n",
    "- 가장 성능이 좋은 모델인 `hparams05_gbm_30_inc_estimators`는 `n_estimators == 400`, `max_depth == 6` 인 gbm 30개를 ensemble한 결과였다.\n",
    "- 다양한 hparams 조합을 실험해본 결과, 실험 01과는 조금 다른 양상을 확인할 수 있었다.\n",
    "    - `learning_rate`는 0.1일 때 가장 좋은 성능을 보였다. (그 이상인 경우 validation score 감소)\n",
    "    - `max_depth`는 6일 때 가장 좋은 성능을 보였다. (그 이상인 경우 validation score 감소)\n",
    "    - `n_estimators`는 값을 키울수록 성능이 좋아졌다. 다만 일정 개수 이상부터는 성능 변화가 거의 없었다 (하락은 하지 않았다).\n",
    "- Ensemble할 모델의 개수와 `n_estimators` 값을 더 높인다면 성능이 올라갈 것 같기는 하지만, 크게 향상될 것 같다는 생각은 들지 않아서 실험을 마무리하였다.\n",
    "- 학습 데이터에 중복된 데이터가 존재할 수 있다는 점을 망각하여 전처리를 하지 않았었는데, 중복 데이터를 삭제한 뒤 hparams05 세팅으로 실험한 결과 test f1 score가 크게 향상되었음.\n",
    "\n",
    "**정리**\n",
    "\n",
    "`+`\n",
    "- increase n_estimators\n",
    "- increase max_depth\n",
    "---\n",
    "`-`\n",
    "- increase learning_rate\n",
    "\n",
    "**Future works**\n",
    "- xgboost, LightGBM, CatBoost\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:orange\">[0.755676, final]</span></span> 실험 03: `GradientBoostingClassifier()` ensemble with Feature Engineering**\n",
    "\n",
    "**Motivation**\n",
    "- Generalization 성능을 높이기 위해 다양한 feature engineering을 시도하고, 그에 따른 결과를 비교분석하였다.\n",
    "<br><br>\n",
    "\n",
    "**Baseline parameters**\n",
    "```python\n",
    "gbm_hparams = {\n",
    "    'loss': 'log_loss', # The loss function to be optimized.\n",
    "    'learning_rate':0.1, # Learning rate shrinks the contribution of each tree by learning_rate. \n",
    "    'n_estimators': 400, # The number of boosting stages to perform.\n",
    "    'subsample': 1.0, # The fraction of samples to be used for fitting the individual base learners.\n",
    "    'criterion': 'friedman_mse', # The function to measure the quality of a split.\n",
    "    'min_samples_split': 2, # The minimum number of samples required to split an internal node:\n",
    "    'min_samples_leaf': 1, # The minimum number of samples required to be at a leaf node.\n",
    "    'max_depth': 6, # Maximum depth of the individual regression estimators.\n",
    "    'min_impurity_decrease': 0.0, # A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "    'init': None, # An estimator object that is used to compute the initial predictions.\n",
    "    # 'random_state': hparams['seed'], # Controls the random seed given to each Tree estimator at each boosting iteration.\n",
    "    'max_features': None, # The number of features to consider when looking for the best split:\n",
    "    'verbose': 0, # Enable verbose output.\n",
    "    'max_leaf_nodes': None, # Grow trees with max_leaf_nodes in best-first fashion.\n",
    "    'warm_start': False,\n",
    "    'validation_fraction': 0.1, # The proportion of training data to set aside as validation set for early stopping.\n",
    "    'n_iter_no_change': None, # n_iter_no_change is used to decide if early stopping will be used to terminate training when validation score is not improving.\n",
    "    'tol': 1e-4, # Tolerance for the early stopping.\n",
    "    'ccp_alpha': 0.0 # Complexity parameter used for Minimal Cost-Complexity Pruning.\n",
    "}\n",
    "```\n",
    "<br>\n",
    "\n",
    "**Details**\n",
    "- `EDA/02_feature_engineering.ipynb` 파일을 참고하여 feature engineering을 수행하였다.\n",
    "<br><br>\n",
    "\n",
    "**Test f1 score**\n",
    "- Previous best test f1 score: 0.743392"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>model_name</th>\n",
       "      <th>test_f1_score</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2024-02-26/01:27:22</td>\n",
       "      <td>hparams05_gbm_30_fe02</td>\n",
       "      <td>0.758583</td>\n",
       "      <td>feature engineering 01 + bin size 500으로 증가 + regrouping + hparams05 세팅 + gbm 30개 앙상블</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2024-02-26/11:20:07</td>\n",
       "      <td>hparams05_gbm_30_fe02</td>\n",
       "      <td>0.758583</td>\n",
       "      <td>feature engineering 01 + bin size 500으로 증가 + regrouping + hparams05 세팅 + gbm 30개 앙상블</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2024-02-25/16:25:47</td>\n",
       "      <td>hparams05_gbm_30_del2Features</td>\n",
       "      <td>0.758170</td>\n",
       "      <td>hparams05 세팅 + 중복 데이터 삭제 + customer_country.1/id_strategic_ver/it_strategic_ver feature 삭제 + customer_country에서 국가 정보만 추출 + customer_idx binning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2024-02-25/17:30:39</td>\n",
       "      <td>hparams05_gbm_30_incBinSize</td>\n",
       "      <td>0.756550</td>\n",
       "      <td>hparams05 세팅 + 중복 데이터 삭제 + customer_country.1/id_strategic_ver/it_strategic_ver feature 삭제 + customer_country에서 국가 정보만 추출 + customer_idx bin size를 1000으로 설정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2024-02-26/16:25:00</td>\n",
       "      <td>hparams05_gbm_30_final</td>\n",
       "      <td>0.755676</td>\n",
       "      <td>feature engineering + 30개 gbm ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2024-02-24/23:23:58</td>\n",
       "      <td>hparams05_gbm_30_binning</td>\n",
       "      <td>0.754655</td>\n",
       "      <td>hparams05 세팅 + 중복 데이터 삭제 + customer_country.1 feature 삭제 + customer_country에서 국가 정보만 추출 + customer_idx binning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2024-02-25/16:03:46</td>\n",
       "      <td>hparams05_gbm_30_logTransformation</td>\n",
       "      <td>0.754655</td>\n",
       "      <td>hparams05 세팅 + 중복 데이터 삭제 + customer_country.1 feature 삭제 + customer_country에서 국가 정보만 추출 + customer_idx binning + historical_existing_cnt log transformation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2024-02-26/00:42:11</td>\n",
       "      <td>hparams05_gbm_30_fe01</td>\n",
       "      <td>0.754635</td>\n",
       "      <td>feature engineering 01 + hparams05 세팅 + gbm 30개 앙상블</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2024-02-25/15:45:37</td>\n",
       "      <td>hparams05_gbm_30_regroupCustomerType</td>\n",
       "      <td>0.753247</td>\n",
       "      <td>hparams05 세팅 + 중복 데이터 삭제 + customer_country.1 feature 삭제 + customer_country에서 국가 정보만 추출 + customer_idx binning + customer_type regroup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2024-02-24/21:36:19</td>\n",
       "      <td>hparams05_gbm_30_delFeature_feCustomerCountry</td>\n",
       "      <td>0.751230</td>\n",
       "      <td>hparams05 세팅 + 중복 데이터 삭제 + customer_country.1 feature 삭제 + customer_country에서 국가 정보만 추출</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2024-02-24/21:16:17</td>\n",
       "      <td>hparams05_gbm_30_delFeature</td>\n",
       "      <td>0.743421</td>\n",
       "      <td>hparams05 세팅 + 중복 데이터 삭제 + customer_country.1 feature 삭제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2024-02-24/00:12:24</td>\n",
       "      <td>hparams05_gbm_30_inc_estimators_dropDuplicates</td>\n",
       "      <td>0.743392</td>\n",
       "      <td>hparams05 세팅 + 중복 데이터 (negative 3000개, positive 200개) 삭제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2024-02-24/21:05:02</td>\n",
       "      <td>hparams05_gbm_30_feCustomerCountry</td>\n",
       "      <td>0.741846</td>\n",
       "      <td>hparams05 세팅 + 중복 데이터 삭제 + customer_country feature 정규화 및 customer_country.1 feature 삭제</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   date                                      model_name  \\\n",
       "55  2024-02-26/01:27:22                           hparams05_gbm_30_fe02   \n",
       "56  2024-02-26/11:20:07                           hparams05_gbm_30_fe02   \n",
       "52  2024-02-25/16:25:47                   hparams05_gbm_30_del2Features   \n",
       "53  2024-02-25/17:30:39                     hparams05_gbm_30_incBinSize   \n",
       "57  2024-02-26/16:25:00                          hparams05_gbm_30_final   \n",
       "49  2024-02-24/23:23:58                        hparams05_gbm_30_binning   \n",
       "51  2024-02-25/16:03:46              hparams05_gbm_30_logTransformation   \n",
       "54  2024-02-26/00:42:11                           hparams05_gbm_30_fe01   \n",
       "50  2024-02-25/15:45:37            hparams05_gbm_30_regroupCustomerType   \n",
       "48  2024-02-24/21:36:19   hparams05_gbm_30_delFeature_feCustomerCountry   \n",
       "47  2024-02-24/21:16:17                     hparams05_gbm_30_delFeature   \n",
       "45  2024-02-24/00:12:24  hparams05_gbm_30_inc_estimators_dropDuplicates   \n",
       "46  2024-02-24/21:05:02              hparams05_gbm_30_feCustomerCountry   \n",
       "\n",
       "    test_f1_score  \\\n",
       "55       0.758583   \n",
       "56       0.758583   \n",
       "52       0.758170   \n",
       "53       0.756550   \n",
       "57       0.755676   \n",
       "49       0.754655   \n",
       "51       0.754655   \n",
       "54       0.754635   \n",
       "50       0.753247   \n",
       "48       0.751230   \n",
       "47       0.743421   \n",
       "45       0.743392   \n",
       "46       0.741846   \n",
       "\n",
       "                                                                                                                                                     description  \n",
       "55                                                                          feature engineering 01 + bin size 500으로 증가 + regrouping + hparams05 세팅 + gbm 30개 앙상블  \n",
       "56                                                                          feature engineering 01 + bin size 500으로 증가 + regrouping + hparams05 세팅 + gbm 30개 앙상블  \n",
       "52              hparams05 세팅 + 중복 데이터 삭제 + customer_country.1/id_strategic_ver/it_strategic_ver feature 삭제 + customer_country에서 국가 정보만 추출 + customer_idx binning  \n",
       "53  hparams05 세팅 + 중복 데이터 삭제 + customer_country.1/id_strategic_ver/it_strategic_ver feature 삭제 + customer_country에서 국가 정보만 추출 + customer_idx bin size를 1000으로 설정  \n",
       "57                                                                                                                        feature engineering + 30개 gbm ensemble  \n",
       "49                                                hparams05 세팅 + 중복 데이터 삭제 + customer_country.1 feature 삭제 + customer_country에서 국가 정보만 추출 + customer_idx binning  \n",
       "51   hparams05 세팅 + 중복 데이터 삭제 + customer_country.1 feature 삭제 + customer_country에서 국가 정보만 추출 + customer_idx binning + historical_existing_cnt log transformation  \n",
       "54                                                                                                           feature engineering 01 + hparams05 세팅 + gbm 30개 앙상블  \n",
       "50                        hparams05 세팅 + 중복 데이터 삭제 + customer_country.1 feature 삭제 + customer_country에서 국가 정보만 추출 + customer_idx binning + customer_type regroup  \n",
       "48                                                                       hparams05 세팅 + 중복 데이터 삭제 + customer_country.1 feature 삭제 + customer_country에서 국가 정보만 추출  \n",
       "47                                                                                                      hparams05 세팅 + 중복 데이터 삭제 + customer_country.1 feature 삭제  \n",
       "45                                                                                                      hparams05 세팅 + 중복 데이터 (negative 3000개, positive 200개) 삭제  \n",
       "46                                                                       hparams05 세팅 + 중복 데이터 삭제 + customer_country feature 정규화 및 customer_country.1 feature 삭제  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.read_csv(\"../experimental_records.csv\")\n",
    "df = df[df['model_name'].apply(lambda x: True if x.find('gbm') + 1 else False)].loc[45:]\n",
    "df.sort_values(by='test_f1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "- 이전에 가장 높은 test f1 score를 기록했던 45번 모델을 기준으로 했을 때, feature engineering을 추가할수록 test f1 score가 높아지는 것을 확인할 수 있었다.\n",
    "- 47번 모델 (`hparams05_gbm_30_delFeature`) 의 경우 45번 모델에서 `customer_country.1` feature를 삭제한 뒤 학습시킨 모델이다. 삭제 이유는 `customer_country` feature와 correlation을 계산했을 때 1에 가까운 값이 나올 정도로 연관성이 높았기 때문이었다. 실제로 해당 feature를 삭제한 데이터를 학습한 모델의 test f1 score가 소폭 향상되는 것을 확인할 수 있었다.\n",
    "- 48번 모델은 47번 모델에서 `customer_country` feature에 engineering을 가한 모델이다. EDA를 통해 `customer_country` feature의 값들이 너무 다양하다는 것을 확인하였고, 그에 따라 overfitting 문제가 발생할 수 있다고 판단하여 원본 값에서 국가명만을 추출하는 방식으로 engineering을 적용했다. FE를 통해 15000개 이상의 고유값을 수 백개 단위로 줄일 수 있었고, test f1 score 또한 크게 향상되는 것을 확인할 수 있었다.\n",
    "- 49번 모델은 48번 모델에서 `customer_idx` feature에 binning을 적용한 모델이다. `customer_idx` feature의 경우 target value를 예측하는 것에 있어 중요도가 높은 feature인 것을 feature importance 시각화를 통해 파악했다. 허나 고유값의 종류가 35000개에 달할 정도로 너무 세분화가 되어 있어 overfitting 문제가 발생할 수 있다는 생각이 들었고, 구간 별로 값을 묶어주는 binning을 통해 고유값의 종류를 줄임으로써 혹시 모를 overfitting 문제를 해소하고자 하였다. 구간의 범위는 100으로 설정한 뒤 binning을 적용하였고, 이를 학습한 모델의 test f1 score를 측정한 결과 꽤 크게 성능이 향상되는 것을 확인할 수 있었다.\n",
    "- 50번 모델의 경우, `customer_type` feature의 값들을 regroup한 데이터를 학습한 모델이다. Raw data를 뜯어보았을 때 대/소문자, 오타, 특수문자, 언어의 차이 등에 의해 값들이 파편화된 상태였기 때문에, 등장 빈도가 높은 값들을 기준으로 regroup을 진행한 뒤 모델을 학습시켰다. 허나 test f1 score는 오히려 하락하였는데, 아마도 public test data의 경우 regroup 이전의 정보가 target value 예측에 도움이 되었던 게 아닐까 생각했다. Test score가 하락하긴 했지만, 나중에 있을 private score를 생각한다면 regroup을 진행하는 것이 맞다고 생각하였고, 이후 대부분의 실험에서도 regroup을 사용하게 되었다.\n",
    "- 51번 모델은 49번 모델의 세팅에서 log transformation을 적용한 모델이다. 일부 feature의 경우 skewed distribution을 가진 상태여서, log transformation을 통해 분포를 완만하게 만들어줌으로써 test score 향상을 꾀했다. 실제로 성능 향상은 없었으나, regroup처럼 private score를 생각해서 log transformation은 이후 대다수의 실험에서 사용하였다.\n",
    "- 52번 모델은 49번 모델의 세팅에서 feature 2개를 추가적으로 삭제한 모델이다. 삭제한 2개의 features는 결측치 비율이 매우 높고 (90% 이상) 다른 feature와의 correlation 또한 높았기 때문에 (0.6) 제거하게 되었다. 모델의 hyperparameters 세팅은 동일한데 학습에 사용하는 features는 정제되었기 때문에 test f1 score가 크게 향상된 것으로 분석하였다.\n",
    "- 53번 모델의 경우 52번 모델의 세팅에서 bin size를 5배 증가시킨 모델이다. Binning을 통해 고유값을 줄였음에도 수 백 개 정도의 고유값이 있었는데, 여전히 너무 많다고 생각해서 bin size를 좀 더 늘려보았다. Test f1 score는 하락하였다.\n",
    "- 54, 55, 56번 모델은 이전 실험들 중에서 성능 향상을 보여주었던 fe 기법들만 모아 실험한 것으로, 최고 점수를 기록하였다.\n",
    "- 57번 모델은 가장 마지막에 제출한 모델이다. `EDA/02_feature_engineering.ipynb` 파일을 기반으로 모든 fe를 적용하였으며, public test score의 경우 하락한 모습이다. Test score가 하락했음에도 해당 모델을 제출한 이유는 private score의 향상을 생각했기 때문이다.\n",
    "<br><br>\n",
    "\n",
    "**정리**\n",
    "\n",
    "`+`\n",
    "- only extract required informations (normalize, regroup, etc)\n",
    "- binning\n",
    "- delete unuseful features\n",
    "<br><br>\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lgaimers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
