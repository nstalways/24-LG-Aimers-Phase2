{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic library\n",
    "import numpy as np\n",
    "\n",
    "# model\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# sampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# custom modules\n",
    "from utils import set_seed, get_clf_eval, make_submission, record_experimental_results\n",
    "import preprocessing as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'seed': 33\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(hparams['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_hparams = {\n",
    "    'loss': 'log_loss', # The loss function to be optimized.\n",
    "    'learning_rate':0.1, # Learning rate shrinks the contribution of each tree by learning_rate. \n",
    "    'n_estimators': 100, # The number of boosting stages to perform.\n",
    "    'subsample': 1.0, # The fraction of samples to be used for fitting the individual base learners.\n",
    "    'criterion': 'friedman_mse', # The function to measure the quality of a split.\n",
    "    'min_samples_split': 2, # The minimum number of samples required to split an internal node:\n",
    "    'min_samples_leaf': 1, # The minimum number of samples required to be at a leaf node.\n",
    "    'max_depth': 3, # Maximum depth of the individual regression estimators.\n",
    "    'min_impurity_decrease': 0.0, # A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "    'init': None, # An estimator object that is used to compute the initial predictions.\n",
    "    # 'random_state': hparams['seed'], # Controls the random seed given to each Tree estimator at each boosting iteration.\n",
    "    'max_features': None, # The number of features to consider when looking for the best split:\n",
    "    'verbose': 0, # Enable verbose output.\n",
    "    'max_leaf_nodes': None, # Grow trees with max_leaf_nodes in best-first fashion.\n",
    "    'warm_start': False,\n",
    "    'validation_fraction': 0.1, # The proportion of training data to set aside as validation set for early stopping.\n",
    "    'n_iter_no_change': None, # n_iter_no_change is used to decide if early stopping will be used to terminate training when validation score is not improving.\n",
    "    'tol': 1e-4, # Tolerance for the early stopping.\n",
    "    'ccp_alpha': 0.0 # Complexity parameter used for Minimal Cost-Complexity Pruning.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실험 01: `GradientBoostingClassifier()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load & label encoding\n",
    "tr_data, tt_data = pp.load_data()\n",
    "x_tr, x_tt = pp.label_encoding(tr_data, tt_data)\n",
    "x_tr, y_tr, x_val, y_val = pp.split_train_and_validation(x_tr, seed=hparams['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "gbm = GradientBoostingClassifier(**gbm_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "gbm.fit(x_tr.fillna(0), y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check validation score\n",
    "y_val_pred = gbm.predict(x_val.fillna(0))\n",
    "get_clf_eval(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "x_tt = x_tt.drop(['is_converted', 'id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = gbm.predict(x_tt.fillna(0))\n",
    "sum(y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실험 02: `GradientBoostingClassifier()` ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = 30 # ensemble할 모델 개수\n",
    "test_results = np.zeros((num_models, 5271)) # 모델별 test 결과를 저장할 배열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_hparams02 = gbm_hparams.copy()\n",
    "gbm_hparams02['ccp_alpha'] = 0.0004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_hparams03 = gbm_hparams.copy()\n",
    "gbm_hparams03['max_depth'] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_hparams04 = gbm_hparams03.copy()\n",
    "gbm_hparams04['n_estimators'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_hparams05 = gbm_hparams04.copy()\n",
    "gbm_hparams05['n_estimators'] = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble loop\n",
    "val_precision, val_recall, val_f1 = [], [], []\n",
    "\n",
    "# data load & drop dupliccates & label encoding\n",
    "tr_data, tt_data = pp.load_data()\n",
    "tr_data.drop_duplicates(inplace=True)\n",
    "tr_data, tt_data = pp.label_encoding(tr_data, tt_data)\n",
    "x_tt = tt_data.drop(['is_converted', 'id'], axis=1)\n",
    "\n",
    "for i in range(num_models):\n",
    "    # 서로 다른 seed를 이용하여 undersampling 수행\n",
    "    rus = RandomUnderSampler(random_state=hparams['seed'] + i)\n",
    "    x_tr_res, y_tr_res = rus.fit_resample(tr_data.drop(['is_converted'], axis=1), tr_data['is_converted'])\n",
    "\n",
    "    # train / validation split\n",
    "    x_tr_res['is_converted'] = y_tr_res # concat\n",
    "    x_tr, y_tr, x_val, y_val = pp.split_train_and_validation(x_tr_res, seed=hparams['seed'])\n",
    "\n",
    "    # define a model\n",
    "    model = GradientBoostingClassifier(**gbm_hparams05, \n",
    "                               random_state=hparams['seed'] + i)\n",
    "\n",
    "    # training\n",
    "    model.fit(x_tr.fillna(0), y_tr)\n",
    "\n",
    "    # test\n",
    "    y_test_pred = model.predict(x_tt.fillna(0))\n",
    "\n",
    "    # 예측 결과를 array에 누적\n",
    "    test_results[i, :] = y_test_pred\n",
    "    \n",
    "\n",
    "    ### print result of current model ###\n",
    "    print('-' * 20)\n",
    "    print(f'Model {i + 1} results')\n",
    "    print('-' * 20)\n",
    "\n",
    "    print(f'current seed: {hparams[\"seed\"] + i}')\n",
    "\n",
    "    # check validation score\n",
    "    y_val_pred = model.predict(x_val.fillna(0))\n",
    "    pr, re, f1 = get_clf_eval(y_val, y_val_pred, is_return=True)\n",
    "    \n",
    "    val_precision.append(pr)\n",
    "    val_recall.append(re)\n",
    "    val_f1.append(f1)\n",
    "\n",
    "    # number of positive predictions\n",
    "    print(sum(y_test_pred))\n",
    "    print()\n",
    "\n",
    "print(f'average validation precision score of {num_models} models: {sum(val_precision) / num_models:.6f}')\n",
    "print(f'average validation recall score of {num_models} models: {sum(val_recall) / num_models:.6f}')\n",
    "print(f'average validation f1 score of {num_models} models: {sum(val_f1) / num_models:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard voting -> 모델별 예측 결과 (1 또는 0) 를 모두 더한 뒤, 합이 int(num_models / 2) + 1 이상이면 1 (positive), 미만이면 0 (negative) 로 예측\n",
    "tmp = np.sum(test_results, axis=0, dtype=int)\n",
    "final_test_pred = np.array([1 if x >= int(num_models / 2) + 1 else 0 for x in tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(final_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'hparams05_gbm_30_inc_estimators_dropDuplicates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(dir_name='05_gbm',\n",
    "                y_pred=final_test_pred,\n",
    "                model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Record**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_experimental_results(model_name=model_name,\n",
    "                            test_f1_score='0.7433920704845814',\n",
    "                            description='hparams05 세팅 + 중복 데이터 (negative 3000개, positive 200개) 삭제')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lgaimers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
