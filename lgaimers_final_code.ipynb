{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c21b65ac",
   "metadata": {},
   "source": [
    "### 필요한 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c83ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn==0.12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5562457d",
   "metadata": {},
   "source": [
    "### 필요한 라이브러리 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc8838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in library\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# basic library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "\n",
    "# model\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# sampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4397180",
   "metadata": {},
   "source": [
    "### 전체적으로 사용할 hyperparameters선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7515907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {'seed': 33, 'num_ensemble': 30}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bf5ef4",
   "metadata": {},
   "source": [
    "### 성능 재현을 위한 시드 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6eacb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 2024) -> None:\n",
    "    \"\"\"실험 재현을 위해 seed를 설정하는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): 설정할 seed 값. Defaults to 2024.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f655b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(hparams['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad8c77",
   "metadata": {},
   "source": [
    "### 데이터 전처리에 필요한 함수들 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f267b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding(tr_data: pd.DataFrame, tt_data: pd.DataFrame,\n",
    "                   features: list = [\"customer_country\", \"business_subarea\",\n",
    "                                     \"business_area\", \"business_unit\", \"customer_type\",\n",
    "                                     \"enterprise\", \"customer_job\",\n",
    "                                     \"inquiry_type\", \"product_category\",\n",
    "                                     \"product_subcategory\", \"product_modelname\",\n",
    "                                     \"customer_country.1\", \"customer_position\",\n",
    "                                     \"response_corporate\",\"expected_timeline\"]) -> tuple:\n",
    "    \"\"\"범주형 데이터를 수치형 데이터로 encoding 하는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        tr_data (pd.DataFrame): 학습 데이터입니다.\n",
    "        tt_data (pd.DataFrame): 테스트 데이터입니다.\n",
    "        features (list, optional): 학습 데이터의 feature 중 범주형 features 의 이름을 담은 리스트입니다.\n",
    "\n",
    "    Returns:\n",
    "        tuple: label encoding을 마친 train, test DataFrame을 반환합니다.\n",
    "    \"\"\"\n",
    "    # train / test data 복사\n",
    "    x_tr = tr_data.copy()\n",
    "    x_tt = tt_data.copy()\n",
    "\n",
    "    for f in features:\n",
    "        # 데이터 타입이 object (str) 일 때 label encoding 수행\n",
    "        if x_tr[f].dtype.name == 'object':\n",
    "            le = LabelEncoder()\n",
    "\n",
    "            # train + test 데이터를 합쳐서 label encoding\n",
    "            cur_tr_f = list(x_tr[f].values)\n",
    "            cur_tt_f = list(x_tt[f].values)\n",
    "\n",
    "            le.fit(cur_tr_f + cur_tt_f)\n",
    "\n",
    "            x_tr[f] = le.transform(cur_tr_f)\n",
    "            x_tt[f] = le.transform(cur_tt_f)\n",
    "\n",
    "    return x_tr, x_tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0267ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_and_validation(tr_data: pd.DataFrame, val_size: float = 0.2, seed: int = 2024) -> tuple:\n",
    "    \"\"\"주어진 data를 train / validation set으로 나누는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        tr_data (pd.DataFrame): split 할 data 입니다.\n",
    "        val_size (float, optional): validation data의 비율입니다. Defaults to 0.2.\n",
    "        seed (int, optional): sampling 시 사용할 seed 값입니다. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (x_train, y_train, x_validation, y_validation) 을 반환\n",
    "    \"\"\"\n",
    "    \n",
    "    x_tr, x_val, y_tr, y_val = train_test_split(tr_data.drop(columns=['is_converted'], axis=1),\n",
    "                                                tr_data['is_converted'],\n",
    "                                                test_size=val_size,\n",
    "                                                random_state=seed,\n",
    "                                                shuffle=True)\n",
    "    \n",
    "    return (x_tr, y_tr, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5929aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_features(tr_data: pd.DataFrame, tt_data: pd.DataFrame,\n",
    "                    features: list = ['com_reg_ver_win_rate', 'customer_type', 'customer_country.1',\n",
    "                                      'historical_existing_cnt', 'id_strategic_ver',\n",
    "                                      'it_strategic_ver', 'idit_strategic_ver','product_subcategory',\n",
    "                                      'product_modelname', 'expected_timeline', 'ver_win_rate_x',\n",
    "                                      'ver_win_ratio_per_bu', 'business_area','business_subarea']) -> tuple:\n",
    "    \"\"\"\n",
    "    주어진 데이터에서 features 에 속하는 feature column 들을 삭제한 뒤 반환합니다.\n",
    "\n",
    "    Args:\n",
    "        tr_data (pd.DataFrame): training data 입니다.\n",
    "        tt_data (pd.DataFrame): test data 입니다.\n",
    "        features (list, optional): 삭제할 feature list 입니다. \n",
    "        기본값은 결측치 비율이 50 % 이상인 feature 들 + 중복 feature 입니다.\n",
    "\n",
    "    Returns:\n",
    "        tuple: features 를 제거한 (tr_data, tt_data) 를 반환합니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    tr_data = tr_data.drop(columns=features, axis=1)\n",
    "    tt_data = tt_data.drop(columns=features, axis=1)\n",
    "\n",
    "    return (tr_data, tt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a81afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_country_name(tr_data: pd.DataFrame, tt_data: pd.DataFrame) -> tuple:\n",
    "    \"\"\"customer_country feature로부터 국가명을 추출하여\n",
    "    주어진 dataframe의 country라는 새로운 feature에 할당하는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        tr_data (pd.DataFrame): training data 입니다.\n",
    "        tt_data (pd.DataFrame): test data 입니다.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            customer_country, customer_country.1 feature는 삭제되고\n",
    "            country features는 추가된 (tr_data, tt_data) 를 반환합니다.\n",
    "    \"\"\"\n",
    "    for df in [tr_data, tt_data]:\n",
    "        nan_val = df[df.isna()].loc[0][0] # 결측값 가져오기\n",
    "\n",
    "        countries = [] # 추출한 국가명을 저장할 배열\n",
    "        for name in df['customer_country']:\n",
    "            flag = False\n",
    "            try:\n",
    "                name = name.lower()\n",
    "                res = name.split(\"/\")\n",
    "                if re.search(\"@\", res[-1]) or re.search(\"[0-9]\", res[-1]): # 비정상 데이터 예외처리\n",
    "                    flag = True\n",
    "                \n",
    "                else:\n",
    "                    countries.append(res[-1].strip())\n",
    "\n",
    "            except AttributeError: # nan value 예외처리\n",
    "                flag = True\n",
    "\n",
    "            if flag:\n",
    "                countries.append(nan_val)\n",
    "\n",
    "        df['country'] = countries\n",
    "        df.sort_index(axis=1, inplace=True)\n",
    "\n",
    "    # correlation이 높은 customer_country, customer_country.1 feature 삭제\n",
    "    tr_data, tt_data = delete_features(tr_data, tt_data, features=['customer_country', 'customer_country.1'])\n",
    "\n",
    "    return (tr_data, tt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f286b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regroup(tr_data: pd.DataFrame, tt_data: pd.DataFrame, \n",
    "            feature_name: str, regroup_info: List[List],\n",
    "            except_val: str='others', except_thr: int = 1) -> tuple:\n",
    "    \"\"\"regroup_info를 바탕으로 data[feature_name]의 값들을 regroup합니다.\n",
    "\n",
    "    Args:\n",
    "        tr_data (pd.DataFrame): training data입니다.\n",
    "        tt_data (pd.DataFrame): test data입니다.\n",
    "        feature_name (str): regroup을 적용할 feature의 이름입니다.\n",
    "        regroup_info (List[List]): regroup 정보입니다. 각각의 리스트는 하나의 새로운 그룹을 의미합니다.\n",
    "        except_val (str): except_thr 이하만큼 등장하는 값을 처리할 때 사용할 값입니다.\n",
    "        except_thr (int): 최소 등장 횟수입니다.\n",
    "\n",
    "    Returns:\n",
    "        tuple: regroup을 마친 tr_data, tt_data를 반환합니다.\n",
    "    \"\"\"\n",
    "    # 데이터를 연결\n",
    "    data = pd.concat([tr_data, tt_data])\n",
    "\n",
    "    # value별 등장 횟수 사전 생성\n",
    "    freq = data[feature_name].value_counts().to_dict()\n",
    "\n",
    "    # regroup_info를 바탕으로 regroup 수행\n",
    "    regroup_results = []\n",
    "    for val in data[feature_name].values:\n",
    "        if type(val) == float: # 결측치\n",
    "            regroup_results.append(val)\n",
    "            continue\n",
    "        \n",
    "        flag = True\n",
    "        for group_pool in regroup_info:\n",
    "            if val in group_pool:\n",
    "                regroup_results.append(group_pool[0].lower())\n",
    "                flag = False\n",
    "                break\n",
    "        \n",
    "        if flag:\n",
    "            if freq[val] <= except_thr:\n",
    "                regroup_results.append(except_val.lower())\n",
    "            else:\n",
    "                regroup_results.append(val)\n",
    "\n",
    "    # 데이터 분리\n",
    "    data[feature_name] = regroup_results\n",
    "    tr_data, tt_data = data.iloc[:len(tr_data)].drop(['id'], axis=1), data.iloc[len(tr_data):]\n",
    "\n",
    "    return tr_data, tt_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b07eb8",
   "metadata": {},
   "source": [
    "### 그 외 편의성을 위해 사용하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4fdbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(tr_path: str = \"train.csv\",\n",
    "              tt_path: str = \"submission.csv\") -> tuple:\n",
    "    \"\"\"학습 및 테스트 데이터를 불러옵니다.\n",
    "\n",
    "    Args:\n",
    "        tr_path (str, optional): 학습용 데이터의 경로입니다.\n",
    "        tt_path (str, optional): 테스트용 데이터의 경로입니다.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (pd.DataFrame, pd.DataFrame)\n",
    "    \"\"\"\n",
    "    \n",
    "    tr_data = pd.read_csv(tr_path)\n",
    "    tr_data.drop_duplicates(inplace=True)\n",
    "\n",
    "    tt_data = pd.read_csv(tt_path)\n",
    "\n",
    "    return (tr_data, tt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6b23d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clf_eval(y_test: np.ndarray, y_pred: np.ndarray = None, is_return: bool = False):\n",
    "    \"\"\"classifier 평가 결과를 출력하는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        y_test (np.ndarray): 정답 데이터입니다.\n",
    "        y_pred (np.ndarray, optional): 모델의 예측 결과 데이터입니다. Defaults to None.\n",
    "    \"\"\"\n",
    "    confusion = confusion_matrix(y_test, y_pred, labels=[True, False])\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, labels=[True, False])\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, labels=[True, False])\n",
    "\n",
    "    # visualize confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=confusion,\n",
    "                                  display_labels=[True, False])\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n정확도: {:.4f}\".format(accuracy))\n",
    "    print(\"정밀도: {:.4f}\".format(precision))\n",
    "    print(\"재현율: {:.4f}\".format(recall))\n",
    "    print(\"F1: {:.4f}\".format(f1))\n",
    "\n",
    "    if is_return:\n",
    "        return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a089384",
   "metadata": {},
   "source": [
    "### 데이터 전처리\n",
    "1. `extract_country_name()`: `country_name` feature로부터 국가명만을 추출\n",
    "1. `binning`: `customer_idx` feature를 bin 단위로 묶음\n",
    "1. `delete_features()`: 불필요하다고 판단한 features 삭제\n",
    "1. `log transformation`: skewed distribution을 갖는 features에 대해 변환 수행\n",
    "1. `regroup()`: 값이 파편화되어있는 features에 대해 regroup 수행\n",
    "1. `label_encoding()`: 범주형 변수에 대해 encoding 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af86d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기 & 중복 데이터 삭제\n",
    "tr_data, tt_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49400b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 국가명 추출\n",
    "tr_data, tt_data = extract_country_name(tr_data, tt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d0efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning\n",
    "start, stop, step = 0, 47501, 500\n",
    "bins = np.arange(start, stop, step)\n",
    "labels = [i for i in range(len(bins) - 1)]\n",
    "\n",
    "tr_data['customer_idx'] = pd.Series(pd.cut(tr_data['customer_idx'], bins=bins, labels=labels), dtype='int64')\n",
    "tt_data['customer_idx'] = pd.Series(pd.cut(tt_data['customer_idx'], bins=bins, labels=labels), dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe19f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요한 feature 삭제\n",
    "tr_data, tt_data = delete_features(tr_data, tt_data, features=['id_strategic_ver', 'it_strategic_ver', 'product_modelname', 'ver_cus', 'ver_pro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b49235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transformation\n",
    "cols = ['com_reg_ver_win_rate', 'historical_existing_cnt', 'lead_desc_length']\n",
    "for col in cols:\n",
    "    tr_data[col] = tr_data[col].apply(np.log1p)\n",
    "    tt_data[col] = tt_data[col].apply(np.log1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078bdac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regroup\n",
    "regroup_customer_type = [['End-Customer', 'End Customer', 'End-user', 'Commercial end-user'],\n",
    "                         ['Specifier / Influencer', 'Specifier/ Influencer'],\n",
    "                         ['Distributor', 'Dealer/Distributor'],\n",
    "                         ['Installer', 'Installer/Contractor'],\n",
    "                         ['Homeowner', 'Home Owner'],\n",
    "                         ['Others', 'other', 'Etc.', 'Other']]\n",
    "\n",
    "regroup_customer_job = [['engineering', 'engineering & technical', 'technical', 'engineer', 'chief engineer', 'engineering & technical executive'],\n",
    "                        ['others', 'other'],\n",
    "                        ['information technology', 'information_technology'],\n",
    "                        ['operations', 'operations manager'],\n",
    "                        ['business development', 'business_development'],\n",
    "                        ['art and design', 'arts and design', 'kreation_und_design', 'designer', 'arts_and_design'],\n",
    "                        ['program and project management', 'programm-_und_projektmanagement', 'program_and_project_management', 'projektmenedzsment\\tprogram and project management', 'manager', 'project manager', 'general manager', 'it manager', 'operations manager', 'sales manager'],\n",
    "                        ['media and communication', 'media_e_comunicazione'],\n",
    "                        ['healthcare services', 'healthcare_services'],\n",
    "                        ['community and social services', 'community_and_social_services'],\n",
    "                        ['research', 'research & development'],\n",
    "                        ['surgery professional', 'surgery professional\\u200b'],\n",
    "                        ['quality_assurance', 'quality_assurance'],\n",
    "                        ['director', 'it director', 'it', 'director of it'],\n",
    "                        ['ceo/founder', 'ceo'],\n",
    "                        ['architect', 'arquitecto/consultor'],\n",
    "                        ['finance', 'finanzen'],\n",
    "                        ['integrator', 'integrador'],\n",
    "                        ['coordinator', 'project coordinator'],\n",
    "                        ['administrative', 'administrative assistant']]\n",
    "\n",
    "regroup_inquiry_type = [['Quotation or purchase consultation', 'Quotation or Purchase Consultation', 'quotation_or_purchase_consultation', 'Quotation or Purchase consultation', 'quotation_', 'Request for quotation or purchase', 'Purchase or Quotation', 'Purchase'],\n",
    "                        ['Sales Inquiry', 'sales', 'Sales inquiry'],\n",
    "                        ['Usage or technical consultation', 'Technical Consultation', 'Usage or Technical Consultation', 'usage or technical consultation', 'usage_or_technical_consultation', 'technical_consultation', 'Technical Support', 'Request for technical consulting', 'technical'],\n",
    "                        ['Others', 'Other', 'ETC.', 'ETC.', 'Etc.', 'others', 'other', 'other_']]\n",
    "\n",
    "regroup_customer_position = [['others', 'other'],\n",
    "                             ['entry level', 'entrylevel'],\n",
    "                             ['c-level executive', 'c-levelexecutive'],\n",
    "                             ['vice president', 'vicepresident'],\n",
    "                             ['end-user', 'commercial end-user'],\n",
    "                             ['decision maker', 'decision-maker'],\n",
    "                             ['decision influencer', 'decision-influencer']]\n",
    "\n",
    "regroup_expected_timeline = [['less than 3 months', 'less_than_3_months'],\n",
    "                             ['3 months ~ 6 months', '3_months_~_6_months'],\n",
    "                             ['less than 6 months'],\n",
    "                             ['6 months ~ 9 months', '6_months_~_9_months'],\n",
    "                             ['more than a year'],\n",
    "                             ['being followed up', 'being followed up.'],\n",
    "                             ['no requirement', 'the client is not having any requirement hence closig in system. although the details of idb are mailed to client.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a47abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data, tt_data = regroup(tr_data, tt_data, 'customer_type', regroup_customer_type, except_val='others', except_thr=5)\n",
    "tr_data, tt_data = regroup(tr_data, tt_data, 'customer_job', regroup_customer_job, except_val='others', except_thr=5)\n",
    "tr_data, tt_data = regroup(tr_data, tt_data, 'inquiry_type', regroup_inquiry_type, except_val='others', except_thr=2)\n",
    "tr_data, tt_data = regroup(tr_data, tt_data, 'customer_position', regroup_customer_position, except_val='others', except_thr=6)\n",
    "tr_data, tt_data = regroup(tr_data, tt_data, 'expected_timeline', regroup_expected_timeline, except_val='others', except_thr=1)\n",
    "tr_data, tt_data = regroup(tr_data, tt_data, 'product_category', [[]], 'etc.', 5)\n",
    "tr_data, tt_data = regroup(tr_data, tt_data, 'product_subcategory', [[]], 'others.', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d53b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding\n",
    "features = [\"business_subarea\", \"country\", \"business_area\", \"business_unit\", \"customer_type\",\n",
    "            \"enterprise\", \"customer_job\", \"inquiry_type\", \"product_category\", \n",
    "            \"product_subcategory\", \"customer_position\", \"response_corporate\",\"expected_timeline\"]\n",
    "\n",
    "tr_data, tt_data = label_encoding(tr_data, tt_data, features=features)\n",
    "x_tt = tt_data.drop(['is_converted', 'id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d87729",
   "metadata": {},
   "source": [
    "### 학습에 사용할 hyperparameters 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa40cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_hparams = {\n",
    "    'loss': 'log_loss', # The loss function to be optimized.\n",
    "    'learning_rate':0.1, # Learning rate shrinks the contribution of each tree by learning_rate. \n",
    "    'n_estimators': 400, # The number of boosting stages to perform.\n",
    "    'subsample': 1.0, # The fraction of samples to be used for fitting the individual base learners.\n",
    "    'criterion': 'friedman_mse', # The function to measure the quality of a split.\n",
    "    'min_samples_split': 2, # The minimum number of samples required to split an internal node:\n",
    "    'min_samples_leaf': 1, # The minimum number of samples required to be at a leaf node.\n",
    "    'max_depth': 6, # Maximum depth of the individual regression estimators.\n",
    "    'min_impurity_decrease': 0.0, # A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "    'init': None, # An estimator object that is used to compute the initial predictions.\n",
    "    # 'random_state': hparams['seed'], # Controls the random seed given to each Tree estimator at each boosting iteration.\n",
    "    'max_features': None, # The number of features to consider when looking for the best split:\n",
    "    'verbose': 0, # Enable verbose output.\n",
    "    'max_leaf_nodes': None, # Grow trees with max_leaf_nodes in best-first fashion.\n",
    "    'warm_start': False,\n",
    "    'validation_fraction': 0.1, # The proportion of training data to set aside as validation set for early stopping.\n",
    "    'n_iter_no_change': None, # n_iter_no_change is used to decide if early stopping will be used to terminate training when validation score is not improving.\n",
    "    'tol': 1e-4, # Tolerance for the early stopping.\n",
    "    'ccp_alpha': 0.0 # Complexity parameter used for Minimal Cost-Complexity Pruning.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eea4a6",
   "metadata": {},
   "source": [
    "### 학습: Gradient Boosting Models Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b7694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균 validation score 확인을 위해 사용\n",
    "val_precision, val_recall, val_f1 = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f9468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction 결과를 누적할 배열 선언\n",
    "test_results = np.zeros((hparams['num_ensemble'], len(tt_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8831933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble loop\n",
    "for i in range(hparams['num_ensemble']):\n",
    "    # 서로 다른 seed를 이용하여 undersampling 수행\n",
    "    rus = RandomUnderSampler(random_state=hparams['seed'] + i)\n",
    "    x_tr_res, y_tr_res = rus.fit_resample(tr_data.drop(['is_converted'], axis=1), tr_data['is_converted'])\n",
    "\n",
    "    # train / validation split\n",
    "    x_tr_res['is_converted'] = y_tr_res # concat\n",
    "    x_tr, y_tr, x_val, y_val = split_train_and_validation(x_tr_res, seed=hparams['seed'])\n",
    "\n",
    "    # define a model\n",
    "    model = GradientBoostingClassifier(**gbm_hparams, random_state=hparams['seed'] + i)\n",
    "\n",
    "    # training\n",
    "    model.fit(x_tr.fillna(0), y_tr)\n",
    "    \n",
    "    ### print result of current model ###\n",
    "    print('-' * 20)\n",
    "    print(f'Model {i + 1} results')\n",
    "    print('-' * 20)\n",
    "\n",
    "    print(f'current seed: {hparams[\"seed\"] + i}')\n",
    "\n",
    "    # check validation score\n",
    "    y_val_pred = model.predict(x_val.fillna(0))\n",
    "    pr, re, f1 = get_clf_eval(y_val, y_val_pred, is_return=True)\n",
    "    \n",
    "    val_precision.append(pr)\n",
    "    val_recall.append(re)\n",
    "    val_f1.append(f1)\n",
    "\n",
    "    # test\n",
    "    y_test_pred = model.predict(x_tt.fillna(0))\n",
    "\n",
    "    # 예측 결과를 array에 누적\n",
    "    test_results[i, :] = y_test_pred\n",
    "\n",
    "    # number of positive predictions\n",
    "    print(sum(y_test_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428af02a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 전체 모델의 평균 validation precision/recall/f1 score 확인\n",
    "print(f\"average validation precision score of {hparams['num_ensemble']} models: {sum(val_precision) / hparams['num_ensemble']:.6f}\")\n",
    "print(f\"average validation recall score of {hparams['num_ensemble']} models: {sum(val_recall) / hparams['num_ensemble']:.6f}\")\n",
    "print(f\"average validation f1 score of {hparams['num_ensemble']} models: {sum(val_f1) / hparams['num_ensemble']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a1962c",
   "metadata": {},
   "source": [
    "### submission file 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f856efa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard voting\n",
    "tmp = np.sum(test_results, axis=0, dtype=int)\n",
    "final_test_pred = np.array([1 if x >= int(hparams['num_ensemble'] / 2) + 1 else 0 for x in tmp])\n",
    "sum(final_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0e1bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(\"submission.csv\")\n",
    "df_sub['is_converted'] = final_test_pred\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
